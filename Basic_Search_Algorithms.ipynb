{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Search Algorithms"
      ],
      "metadata": {
        "id": "F6ZnUt-VWvvd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Manual Search:** Best for small, simple models or when expert knowledge is available; less computational overhead but may miss optimal configurations.\n",
        "\n",
        "**Grid Search:** Comprehensive and systematic but computationally expensive; suitable for smaller hyperparameter spaces.\n",
        "\n",
        "**Random Search:** Efficient for high-dimensional spaces; balances exploration and computational efficiency, especially when the optimal configuration is not well-known."
      ],
      "metadata": {
        "id": "R6Z_L57LWohZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Grid Search"
      ],
      "metadata": {
        "id": "vO6neabFW2kU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qCEDkAlDV4_5"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "from sklearn.model_selection import (\n",
        "    GridSearchCV,\n",
        "    train_test_split,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Set up the model\n",
        "# Initializes a Gradient Boosting Classifier with a fixed random state for reproducibility\n",
        "gbm = GradientBoostingClassifier(random_state=0)\n",
        "\n",
        "# Step 2: Determine the hyperparameter space\n",
        "# Defines the hyperparameter grid with options for number of estimators, min samples split, and max depth\n",
        "param_grid = dict(\n",
        "    n_estimators=[10, 20, 50, 100],          # Number of boosting stages\n",
        "    min_samples_split=[0.1, 0.3, 0.5],       # Minimum fraction of samples required to split a node\n",
        "    max_depth=[1, 2, 3, 4, None],            # Maximum depth of the individual trees\n",
        ")\n",
        "\n",
        "# Step 3: Calculate and print the total number of hyperparameter combinations\n",
        "# Calculates and prints the total number of hyperparameter combinations to be evaluated\n",
        "print('Number of hyperparam combinations: ',\n",
        "      len(param_grid['n_estimators']) * len(param_grid['min_samples_split']) * len(param_grid['max_depth']))\n",
        "\n",
        "# Step 4: Set up the search\n",
        "# Configures Grid Search with the model, hyperparameters, ROC AUC scoring, and 5-fold cross-validation\n",
        "search = GridSearchCV(gbm, param_grid, scoring='roc_auc', cv=5, refit=True)\n",
        "\n",
        "# Step 5: Find best hyperparameters\n",
        "# Fits the model to the training data to find the best hyperparameters\n",
        "search.fit(X_train, y_train)\n",
        "\n",
        "# Step 6: Retrieve the best hyperparameters\n",
        "# Retrieves the best hyperparameters identified by the Grid Search\n",
        "search.best_params_\n",
        "\n",
        "# Step 7: Convert the search results into a DataFrame\n",
        "# Converts the full results of the Grid Search into a DataFrame\n",
        "results = pd.DataFrame(search.cv_results_)\n",
        "\n",
        "# Step 8: Print the shape of the results DataFrame\n",
        "# Prints the shape of the DataFrame containing all model evaluations\n",
        "print(results.shape)\n",
        "\n",
        "# Step 9: Display the first few rows of the results DataFrame\n",
        "# Displays the first few rows of the DataFrame\n",
        "results.head()\n",
        "\n",
        "# Step 10: Sort the models based on performance\n",
        "# Sorts the DataFrame by the mean test score in descending order to find the best performing models\n",
        "results.sort_values(by='mean_test_score', ascending=False, inplace=True)\n",
        "\n",
        "# Step 11: Reset the index of the DataFrame after sorting\n",
        "# Resets the index of the DataFrame after sorting\n",
        "results.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Step 12: Display the top performing models\n",
        "# Displays the top performing models with their parameters and scores\n",
        "results[[\n",
        "    'param_max_depth', 'param_min_samples_split', 'param_n_estimators',\n",
        "    'mean_test_score', 'std_test_score',\n",
        "]].head()\n",
        "\n",
        "# Step 13: Display the bottom performing models\n",
        "# Displays the bottom performing models with their parameters and scores\n",
        "results[[\n",
        "    'param_max_depth', 'param_min_samples_split', 'param_n_estimators',\n",
        "    'mean_test_score', 'std_test_score',\n",
        "]].tail()"
      ],
      "metadata": {
        "id": "LwQ5KHyEW_VZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "0VqEVn9ieiMA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# let's make a function to evaluate the model performance based on\n",
        "# single hyperparameters\n",
        "# Defines a function to summarize the model's performance (mean and std) for a specific hyperparameter\n",
        "def summarize_by_param(hparam):\n",
        "\n",
        "    # Groups the results by the specified hyperparameter and calculates the mean and standard deviation of test scores\n",
        "    tmp = pd.concat([\n",
        "        results.groupby(hparam)['mean_test_score'].mean(),\n",
        "        results.groupby(hparam)['mean_test_score'].std(),\n",
        "    ], axis=1)\n",
        "\n",
        "    # Renames the columns for clarity\n",
        "    tmp.columns = ['mean_test_score', 'std_test_score']\n",
        "\n",
        "    return tmp  # Returns the summary DataFrame\n",
        "\n",
        "# performance change for n_estimators\n",
        "# Evaluates the performance for different values of 'n_estimators'\n",
        "tmp = summarize_by_param('param_n_estimators')\n",
        "\n",
        "tmp.head()  # Displays the first few rows of the summarized results\n",
        "\n",
        "# Plots the mean test score for 'n_estimators' with error bars representing standard deviation\n",
        "tmp['mean_test_score'].plot(yerr=[tmp['std_test_score'], tmp['std_test_score']], subplots=True)\n",
        "plt.ylabel('roc-auc')  # Labels the y-axis to show ROC AUC\n",
        "\n",
        "# performance change for max_depth\n",
        "# Evaluates the performance for different values of 'max_depth'\n",
        "tmp = summarize_by_param('param_max_depth')\n",
        "\n",
        "# Plots the mean test score for 'max_depth' with error bars representing standard deviation\n",
        "tmp['mean_test_score'].plot(yerr=[tmp['std_test_score'], tmp['std_test_score']], subplots=True)\n",
        "plt.ylabel('roc-auc')  # Labels the y-axis to show ROC AUC\n",
        "\n",
        "# performance change for min_samples_split\n",
        "# Evaluates the performance for different values of 'min_samples_split'\n",
        "tmp = summarize_by_param('param_min_samples_split')\n",
        "\n",
        "# Plots the mean test score for 'min_samples_split' with error bars representing standard deviation\n",
        "tmp['mean_test_score'].plot(yerr=[tmp['std_test_score'], tmp['std_test_score']], subplots=True)"
      ],
      "metadata": {
        "id": "dqLAPa_Heisw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once receive the 1st itertion of your best hyper parameters, then you should use them to find the next batch using the given values or do further experiments"
      ],
      "metadata": {
        "id": "V62ZaYb_W2sD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# determine the hyperparameter space\n",
        "param_grid = dict(\n",
        "    n_estimators=[60, 80, 100, 120],\n",
        "    max_depth=[2,3],\n",
        "    loss = ['log_loss', 'exponential'],\n",
        "    )"
      ],
      "metadata": {
        "id": "NH53MISPgsTp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Nested hyper parameter spaces"
      ],
      "metadata": {
        "id": "GyAZeBDjKkVW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set up the model\n",
        "svm = SVC(random_state=0)\n",
        "\n",
        "# determine the hyperparameter space\n",
        "param_grid = [\n",
        "  {'C': [1, 10, 100, 1000], 'kernel': ['linear']},\n",
        "  {'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']},\n",
        " ]"
      ],
      "metadata": {
        "id": "L42H1vYsKfzu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Search"
      ],
      "metadata": {
        "id": "k7wMglZsHfaT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Assumptions**\n",
        "\n",
        "**Random Search Considerations:**\n",
        "* Choosing a Computational Budget:\n",
        "The computational budget (e.g., number of iterations, time spent) is chosen independently (or mostly independently) of the number of hyperparameters and possible values. This gives flexibility in managing resources.\n",
        "\n",
        "**Efficiency with Non-Influential Parameters:**\n",
        "* Adding hyperparameters that do not influence the model's performance does not reduce the efficiency of the search, provided that enough iterations are performed. Random Search naturally focuses more on important parameters during the search process.\n",
        "\n",
        "**Continuous Distributions for Hyperparameters:**\n",
        "* To take full advantage of Random Search’s random sampling, it is crucial to specify a continuous distribution for the hyperparameters. This ensures that Random Search explores the space more effectively rather than being limited to a small set of predefined values."
      ],
      "metadata": {
        "id": "cPpzPVt-Pd7E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sklearn"
      ],
      "metadata": {
        "id": "IcXDj0YgNy_d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "from sklearn.model_selection import (\n",
        "    RandomizedSearchCV,\n",
        "    train_test_split,\n",
        ")"
      ],
      "metadata": {
        "id": "MfOIPHp0HggL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Set up the model\n",
        "# Initializes a Gradient Boosting Classifier with a fixed random state for reproducibility\n",
        "gbm = GradientBoostingClassifier(random_state=0)\n",
        "\n",
        "# Step 2: Determine the hyperparameter space\n",
        "# Defines a hyperparameter grid with random distributions for the number of estimators,\n",
        "# min samples split, max depth, and loss function\n",
        "param_grid = dict(\n",
        "    n_estimators=stats.randint(10, 120),           # Random integers for number of boosting stages\n",
        "    min_samples_split=stats.uniform(0, 1),         # Uniform distribution for minimum fraction of samples to split a node\n",
        "    max_depth=stats.randint(1, 5),                 # Random integers for maximum depth of trees\n",
        "    loss=('log_loss', 'exponential'),              # Categorical options for loss function\n",
        ")\n",
        "\n",
        "# Step 3: Set up the randomized search\n",
        "# Configures Randomized Search with the model, hyperparameters, ROC AUC scoring, 5-fold cross-validation,\n",
        "# and 60 iterations for random search\n",
        "search = RandomizedSearchCV(gbm,\n",
        "                            param_grid,\n",
        "                            scoring='roc_auc',\n",
        "                            cv=5,                # 5-fold cross-validation\n",
        "                            n_iter=60,           # Number of random iterations\n",
        "                            random_state=10,     # Seed for reproducibility\n",
        "                            n_jobs=4,            # Parallelize across 4 jobs\n",
        "                            refit=True)          # Refits best model to entire dataset\n",
        "\n",
        "# Step 4: Find best hyperparameters\n",
        "# Fits the model to the training data to find the best hyperparameters\n",
        "search.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Retrieve best hyperparameters\n",
        "# The best hyperparameters are stored in an attribute\n",
        "search.best_params_\n",
        "\n",
        "# Step 6: Find the data for all models evaluated\n",
        "# Converts the full results of the Randomized Search into a DataFrame\n",
        "results = pd.DataFrame(search.cv_results_)\n",
        "\n",
        "# Step 7: Print the shape of the DataFrame\n",
        "# Displays the shape of the results DataFrame\n",
        "print(results.shape)\n",
        "\n",
        "# Step 8: Display the first few rows of the DataFrame\n",
        "# Shows the first few rows of the DataFrame\n",
        "results.head()\n"
      ],
      "metadata": {
        "id": "yj4Fu4XzLZqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Scikit-Optimize"
      ],
      "metadata": {
        "id": "miaSJ3YtOC0A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Procedure**\n",
        "\n",
        "To tune the hyper-parameters of our model we need to:\n",
        "\n",
        "* define a model\n",
        "* decide which parameters to optimize\n",
        "* define the objective function we want to minimize.\n",
        "\n",
        "**NOTE**\n",
        "\n",
        "Scikit-Optimize will always minimize the objective function, so if we want to maximize a function, for example the roc-auc, we need to negate the metric. Thus, instead of maximizing the roc-auc, we minimize the -roc-auc."
      ],
      "metadata": {
        "id": "-gaUm2YmLZO_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from skopt import dummy_minimize # for the randomized search\n",
        "from skopt.plots import plot_convergence\n",
        "from skopt.space import Real, Integer, Categorical\n",
        "from skopt.utils import use_named_args"
      ],
      "metadata": {
        "id": "y3a4kJpAOcom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Determine the hyperparameter space\n",
        "# Defines a hyperparameter grid for Scikit-Optimize with integer, real, and categorical distributions\n",
        "param_grid = [\n",
        "    Integer(10, 120, name=\"n_estimators\"),           # Integer range for the number of boosting stages\n",
        "    Real(0, 0.999, name=\"min_samples_split\"),        # Real range for the minimum fraction of samples to split a node\n",
        "    Integer(1, 5, name=\"max_depth\"),                 # Integer range for maximum depth of trees\n",
        "    Categorical(['log_loss', 'exponential'], name=\"loss\"),  # Categorical options for loss function\n",
        "]\n",
        "\n",
        "# Step 2: Check the type of the hyperparameter grid\n",
        "# Verifies that the parameter grid is a list\n",
        "type(param_grid)\n",
        "\n",
        "# Step 3: Set up the Gradient Boosting Classifier\n",
        "# Initializes a Gradient Boosting Classifier with a fixed random state for reproducibility\n",
        "gbm = GradientBoostingClassifier(random_state=0)"
      ],
      "metadata": {
        "id": "W6T-ClJ-QOAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Design a function to maximize accuracy using GBM with cross-validation\n",
        "# The decorator `use_named_args` allows the objective function to receive hyperparameters as keyword arguments\n",
        "@use_named_args(param_grid)\n",
        "def objective(**params):\n",
        "\n",
        "    # Step 2: Update the GBM model with new parameters\n",
        "    # Set the parameters of the Gradient Boosting Model to the current set of hyperparameters\n",
        "    gbm.set_params(**params)\n",
        "\n",
        "    # Step 3: Cross-validation to evaluate the model\n",
        "    # Perform 3-fold cross-validation on the training data to compute the accuracy for the current parameters\n",
        "    value = np.mean(\n",
        "        cross_val_score(\n",
        "            gbm,\n",
        "            X_train,        # Training features\n",
        "            y_train,        # Training labels\n",
        "            cv=3,           # 3-fold cross-validation\n",
        "            n_jobs=-4,      # Parallel processing with 4 jobs\n",
        "            scoring='accuracy')  # Scoring based on accuracy\n",
        "    )\n",
        "\n",
        "    # Step 4: Negate the value since the optimizer minimizes the objective\n",
        "    # Return the negative of the accuracy as the objective function minimizes the value\n",
        "    return -value"
      ],
      "metadata": {
        "id": "A-2VMfQ8Q1D0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Perform the randomized search using dummy_minimize\n",
        "# Runs a minimization search to find the best parameters by calling the objective function multiple times\n",
        "search = dummy_minimize(\n",
        "    objective,    # The objective function to minimize\n",
        "    param_grid,   # The hyperparameter space to explore\n",
        "    n_calls=50,   # Number of evaluations for the objective function\n",
        "    random_state=0,  # Ensures reproducibility\n",
        ")\n",
        "\n",
        "# Step 2: Display the best score (negative of accuracy)\n",
        "# Prints the best objective function value found (note: it is the negative of the accuracy)\n",
        "\"Best score=%.4f\" % search.fun\n",
        "\n",
        "# Step 3: Print the best hyperparameters found\n",
        "# Displays the best hyperparameter values identified during the search\n",
        "print(\"\"\"Best parameters:\n",
        "=========================\n",
        "- n_estimators=%d\n",
        "- min_samples_split=%.6f\n",
        "- max_depth=%d\n",
        "- loss=%s\"\"\" % (search.x[0],  # Best n_estimators\n",
        "                search.x[1],  # Best min_samples_split\n",
        "                search.x[2],  # Best max_depth\n",
        "                search.x[3]))  # Best loss function\n",
        "\n",
        "# Step 4: Plot the convergence of the search\n",
        "# Visualizes the convergence of the objective function value over the iterations\n",
        "plot_convergence(search)"
      ],
      "metadata": {
        "id": "wSFVKCDKQ4BJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Randomized Search with Hyperopt\n"
      ],
      "metadata": {
        "id": "pq_pEcyP6ew-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Procedure**\n",
        "\n",
        "To tune the hyper-parameters of our model we need to:\n",
        "\n",
        "* define a model\n",
        "* decide which parameters to optimize\n",
        "* define the objective function we want to minimize."
      ],
      "metadata": {
        "id": "GA8sbi1a6O86"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb  # Imports the XGBoost library for gradient boosting models\n",
        "from hyperopt import hp, rand, fmin, Trials  # Imports necessary modules from Hyperopt for optimization\n",
        "\n",
        "# hp: define the hyperparameter space\n",
        "# rand: random search\n",
        "# fmin: optimization function\n",
        "# Trials: to evaluate the different searched hyperparameters\n",
        "\n",
        "# Step 1: Set up the hyperparameter space\n",
        "# Defines a dictionary specifying the hyperparameter ranges to search\n",
        "param_grid = {\n",
        "    'n_estimators': hp.quniform('n_estimators', 200, 2500, 100),  # Number of boosting rounds, ranges from 200 to 2500, in steps of 100\n",
        "    'max_depth': hp.uniform('max_depth', 1, 10),  # Maximum depth of the trees, continuous values from 1 to 10\n",
        "    'learning_rate': hp.uniform('learning_rate', 0.01, 0.99),  # Learning rate for model updates, ranges from 0.01 to 0.99\n",
        "    'booster': hp.choice('booster', ['gbtree', 'dart']),  # Choice of booster: 'gbtree' or 'dart'\n",
        "    'gamma': hp.quniform('gamma', 0.01, 10, 0.1),  # Minimum loss reduction for further splits, ranges from 0.01 to 10, in steps of 0.1\n",
        "    'subsample': hp.uniform('subsample', 0.50, 0.90),  # Subsample ratio for the training data, continuous values from 0.50 to 0.90\n",
        "    'colsample_bytree': hp.uniform('colsample_bytree', 0.50, 0.99),  # Subsample ratio for features (columns) used for each tree, from 0.50 to 0.99\n",
        "    'colsample_bylevel': hp.uniform('colsample_bylevel', 0.50, 0.99),  # Subsample ratio for features used at each tree level, from 0.50 to 0.99\n",
        "    'colsample_bynode': hp.uniform('colsample_bynode', 0.50, 0.99),  # Subsample ratio for features used at each node split, from 0.50 to 0.99\n",
        "    'reg_lambda': hp.uniform('reg_lambda', 1, 20)  # L2 regularization term (lambda), from 1 to 20\n",
        "}"
      ],
      "metadata": {
        "id": "GLKdKCvk6lPb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Define the objective function\n",
        "# The objective function takes hyperparameters as input to evaluate model performance\n",
        "def objective(params):\n",
        "\n",
        "    # Step 2: Create a dictionary for hyperparameters\n",
        "    # Constructs a dictionary to map hyperparameter values for the XGBoost model\n",
        "    params_dict = {\n",
        "        'n_estimators': int(params['n_estimators']),  # Converts n_estimators to int, as XGBoost requires integers\n",
        "        'max_depth': int(params['max_depth']),        # Converts max_depth to int, as XGBoost requires integers\n",
        "        'learning_rate': params['learning_rate'],      # Learning rate value from the input parameters\n",
        "        'booster': params['booster'],                  # Booster type (e.g., 'gbtree' or 'dart') from the input parameters\n",
        "        'gamma': params['gamma'],                      # Minimum loss reduction for further splits from the input parameters\n",
        "        'subsample': params['subsample'],              # Subsample ratio of the training data from the input parameters\n",
        "        'colsample_bytree': params['colsample_bytree'],  # Subsample ratio of features for each tree from the input parameters\n",
        "        'colsample_bylevel': params['colsample_bylevel'],  # Subsample ratio of features at each tree level from the input parameters\n",
        "        'colsample_bynode': params['colsample_bynode'],  # Subsample ratio of features at each node from the input parameters\n",
        "        'random_state': 1000,                           # Sets a random state for reproducibility\n",
        "    }\n",
        "\n",
        "    # Step 3: Initialize the XGBoost Classifier\n",
        "    # Initializes the XGBoost classifier with the specified hyperparameters\n",
        "    gbm = xgb.XGBClassifier(**params_dict)\n",
        "\n",
        "    # Step 4: Perform cross-validation\n",
        "    # Evaluates model performance using cross-validation with accuracy scoring\n",
        "    score = cross_val_score(gbm, X_train, y_train,\n",
        "                            scoring='accuracy', cv=5, n_jobs=4).mean()  # Averages the accuracy scores across 5 folds\n",
        "\n",
        "    # Step 5: Negate the score for minimization\n",
        "    # Returns the negative of the accuracy score to minimize it during optimization\n",
        "    return -score"
      ],
      "metadata": {
        "id": "fAuzdCH46nVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Perform minimization with hyperopt\n",
        "# Uses the fmin function to find the best hyperparameters by minimizing the objective function\n",
        "search = fmin(\n",
        "    fn=objective,                                  # The objective function to minimize\n",
        "    space=param_grid,                             # The hyperparameter space defined earlier\n",
        "    max_evals=50,                                 # The maximum number of evaluations to perform\n",
        "    rstate=np.random.default_rng(42),            # Sets the random state for reproducibility\n",
        "    algo=rand.suggest,                            # Specifies the algorithm to use for random search\n",
        ")\n",
        "\n",
        "# Step 2: Check the type of the search result\n",
        "# Retrieves the type of the search result, which should be a dictionary\n",
        "type(search)\n",
        "\n",
        "# Step 3: Display the best parameters found by fmin\n",
        "# Displays the dictionary of the best hyperparameters found\n",
        "search\n",
        "\n",
        "# Step 4: Create a dictionary for the best hyperparameters\n",
        "# Maps the search results to the format required for the XGBoost model\n",
        "best_hp_dict = {\n",
        "    'n_estimators': int(search['n_estimators']),  # Converts n_estimators to int, as XGBoost requires integers\n",
        "    'max_depth': int(search['max_depth']),        # Converts max_depth to int, as XGBoost requires integers\n",
        "    'learning_rate': search['learning_rate'],      # Learning rate value from the search results\n",
        "    'booster': 'gbtree',                          # Specifies the booster type\n",
        "    'gamma': search['gamma'],                      # Minimum loss reduction for further splits from the search results\n",
        "    'subsample': search['subsample'],              # Subsample ratio of the training data from the search results\n",
        "    'colsample_bytree': search['colsample_bytree'],  # Subsample ratio of features for each tree from the search results\n",
        "    'colsample_bylevel': search['colsample_bylevel'],  # Subsample ratio of features at each tree level from the search results\n",
        "    'colsample_bynode': search['colsample_bynode'],  # Subsample ratio of features at each node from the search results\n",
        "    'random_state': 1000,                          # Sets a random state for reproducibility\n",
        "}\n",
        "\n",
        "# Step 5: Initialize the final XGBoost Classifier\n",
        "# Creates an XGBoost model using the best hyperparameters obtained from the search\n",
        "gbm_final = xgb.XGBClassifier(**best_hp_dict)\n",
        "\n",
        "# Step 6: Train the final model\n",
        "# Fits the model to the training data using the best hyperparameters\n",
        "gbm_final.fit(X_train, y_train)\n",
        "\n",
        "# Step 7: Make predictions on the training set\n",
        "# Predicts the target values for the training set\n",
        "X_train_preds = gbm_final.predict(X_train)\n",
        "\n",
        "# Step 8: Make predictions on the test set\n",
        "# Predicts the target values for the test set\n",
        "X_test_preds = gbm_final.predict(X_test)\n",
        "\n",
        "# Step 9: Calculate and print training accuracy\n",
        "# Computes and displays the accuracy of the model on the training set\n",
        "print('Train accuracy: ', accuracy_score(y_train, X_train_preds))\n",
        "\n",
        "# Step 10: Calculate and print test accuracy\n",
        "# Computes and displays the accuracy of the model on the test set\n",
        "print('Test accuracy: ', accuracy_score(y_test, X_test_preds))"
      ],
      "metadata": {
        "id": "0DKglVnz6pGm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Trial\n",
        "\n",
        "We can use Trials if we want to look into the search, and the performance values encountered during the process.\n",
        "\n"
      ],
      "metadata": {
        "id": "R2b5WrZJE919"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Initialize Trials\n",
        "# Creates an empty Trials object to store the results of the hyperparameter optimization process\n",
        "trials = Trials()\n",
        "\n",
        "# Step 2: Execute Hyperparameter Optimization\n",
        "# Uses fmin to find the best hyperparameters by minimizing the objective function\n",
        "second_search = fmin(\n",
        "    fn=objective,                         # The objective function to minimize\n",
        "    space=param_grid,                    # The hyperparameter space defined earlier\n",
        "    max_evals=50,                        # Specifies the maximum number of evaluations to perform\n",
        "    rstate=np.random.default_rng(42),    # Sets the random state for reproducibility\n",
        "    algo=rand.suggest,                   # Uses a randomized search algorithm\n",
        "    trials=trials                        # Associates the trials object to track results\n",
        ")\n",
        "\n",
        "# Step 3: View Best Hyperparameters\n",
        "# Displays the best hyperparameters found during the optimization process\n",
        "second_search\n",
        "\n",
        "# Step 4: Retrieve Best Hyperparameters from Trials\n",
        "# Accesses the index of the best trial from the trials object, representing the optimal hyperparameters\n",
        "trials.argmin\n",
        "\n",
        "# Step 5: View Hyperparameter Combinations\n",
        "# Converts the hyperparameter combinations tested during the search into a pandas DataFrame and displays the first few rows\n",
        "pd.DataFrame(trials.vals).head()\n",
        "\n",
        "# Step 6: View Results\n",
        "# Converts the results of each trial into a DataFrame and displays the first few rows\n",
        "pd.DataFrame(trials.results).head()\n",
        "\n",
        "# Step 7: Combine Hyperparameters and Results\n",
        "# Combines hyperparameter combinations and their corresponding results into a single DataFrame, sorted by loss\n",
        "results = pd.concat([\n",
        "    pd.DataFrame(trials.vals),           # DataFrame of hyperparameter values\n",
        "    pd.DataFrame(trials.results)],       # DataFrame of results\n",
        "    axis=1,                               # Concatenate along the columns\n",
        ").sort_values(by='loss', ascending=False).reset_index(drop=True)  # Sort by loss in descending order and reset the index\n",
        "\n",
        "# Step 8: Display Combined Results\n",
        "# Displays the first few rows of the combined results DataFrame\n",
        "results.head()\n",
        "\n",
        "# Step 9: Visualize Loss Values\n",
        "# Plots the loss values against the hyperparameter combinations to visualize performance\n",
        "results['loss'].plot()                    # Plot the loss values\n",
        "plt.ylabel('Accuracy')                    # Label the y-axis (consider changing to 'Loss' if applicable)\n",
        "plt.xlabel('Hyperparam combination')      # Label the x-axis\n",
        "\n",
        "# Step 10: Retrieve Minimum Loss Value\n",
        "# Retrieves and displays the minimum loss value from the results stored in the trials object\n",
        "pd.DataFrame(trials.results)['loss'].min()"
      ],
      "metadata": {
        "id": "g_MHMG3uFFV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How It All Works Together**\n",
        "* **Define Hyperparameter Space**: The param_grid specifies a range of values for different hyperparameters that will be evaluated.\n",
        "* **Objective Function**: The objective function evaluates the model’s performance using a specific combination of hyperparameters.\n",
        "* **Optimization Process**: The fmin function from Hyperopt minimizes the output of the objective function by exploring the param_grid.\n",
        "It samples hyperparameters randomly (due to rand.suggest) and evaluates them by calling the objective function.\n",
        "\n",
        "\n",
        "**Final Model Training**: Once the best hyperparameters are found, they are used to create and train the final model (gbm_final), which is then evaluated for accuracy on the training and test datasets."
      ],
      "metadata": {
        "id": "rcMi7VQK9YiE"
      }
    }
  ]
}