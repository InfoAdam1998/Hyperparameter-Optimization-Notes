{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Scikit-Optimize Definitions"
      ],
      "metadata": {
        "id": "qJ6CSCzhZrO_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Definitions**\n",
        "\n",
        "* Uniform distribution: This means that every value in a certain range has an equal chance of being selected. For example, if you're choosing a learning rate between 0.01 and 0.1, any value in that range is equally likely to be picked.\n",
        "\n",
        "* Log-uniform distribution: This means the values are spread out more on a logarithmic scale. Smaller values are more likely to be chosen than larger ones. For example, for a learning rate, values like 0.001 might be chosen more often than 0.1 because it focuses more on the smaller numbers.\n",
        "\n",
        "Log-uniform is useful when you expect smaller values to be more effective but still want to explore larger ones.\n",
        "\n",
        "**Example**\n",
        "\n",
        "In Artificial Neural Networks (ANNs), we commonly use log-uniform distributions for tuning the learning rate. Here's why:\n",
        "\n",
        "Learning rates often span multiple orders of magnitude (e.g., 0.0001 to 0.1), and using a log-uniform distribution ensures that smaller values, which are typically more effective for fine-tuning, are explored more thoroughly.\n",
        "Since small changes in the learning rate can have a big impact on training, log-uniform allows us to sample more from smaller values, which often leads to better results.\n",
        "In contrast, a uniform distribution would treat all values equally, making it less likely to find those smaller, more effective learning rates. This is why log-uniform is preferred for learning rate in ANN tuning."
      ],
      "metadata": {
        "id": "rqMuyOWKFROZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Basic Syntax"
      ],
      "metadata": {
        "id": "LnB1s0o02Jrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scikit-optimize search space - distributions\n"
      ],
      "metadata": {
        "id": "FphoudZNvmhj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from skopt.space import Real, Integer, Categorical  # Import classes to define hyperparameter spaces\n",
        "from skopt.utils import use_named_args\n",
        "\n",
        "# Step 1: Define an integer hyperparameter space\n",
        "# Integer(10, 120) defines the range of values from 10 to 120 with uniform probability\n",
        "param = Integer(10, 120, prior=\"uniform\", name=\"example\")\n",
        "space = Integer(10, 120, prior=\"uniform\", name=\"example\")\n",
        "\n",
        "# Step 2: Define a continuous real-valued hyperparameter space\n",
        "# Real(0.00001, 0.1) defines the range of values between 0.00001 and 0.1 with uniform probability\n",
        "space = Real(0.00001, 0.1, prior=\"uniform\", name=\"example\")\n",
        "\n",
        "# Log-uniform is useful when smaller values are preferred as it samples based on a logarithmic scale\n",
        "space = Real(0.00001, 0.1, prior=\"log-uniform\", name=\"example\")\n",
        "\n",
        "# Step 3: Define a categorical hyperparameter space\n",
        "# Categorical(['A', 'B', 'C']) defines discrete choices, and one will be selected with equal probability\n",
        "space = Categorical(['A', 'B', 'C'], name=\"example\")\n",
        "\n",
        "# Step 4: Define a complete hyperparameter grid for optimization\n",
        "# param_grid is a list of different hyperparameters with their respective ranges/types\n",
        "param_grid = [\n",
        "    Integer(10, 120, name=\"n_estimators\"),  # Number of trees in the model\n",
        "    Integer(1, 5, name=\"max_depth\"),  # Maximum depth of each tree\n",
        "    Real(0.0001, 0.1, prior='log-uniform', name='learning_rate'),  # Learning rate with log-uniform distribution\n",
        "    Real(0.001, 0.999, prior='log-uniform', name=\"min_samples_split\"),  # Minimum samples split\n",
        "    Categorical(['log_loss', 'exponential'], name=\"loss\"),  # Loss function\n",
        "]"
      ],
      "metadata": {
        "id": "uxjAAfr6ZnKa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Objective Function"
      ],
      "metadata": {
        "id": "wfDIp1TayOLA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Design a function to maximize accuracy using GBM with cross-validation\n",
        "# The decorator `use_named_args` allows the objective function to receive hyperparameters as keyword arguments\n",
        "@use_named_args(param_grid)\n",
        "def objective(**params):\n",
        "\n",
        "    # Step 2: Update the GBM model with new parameters\n",
        "    # Set the parameters of the Gradient Boosting Model to the current set of hyperparameters\n",
        "    gbm.set_params(**params)\n",
        "\n",
        "    # Step 3: Cross-validation to evaluate the model\n",
        "    # Perform 3-fold cross-validation on the training data to compute the accuracy for the current parameters\n",
        "    value = np.mean(\n",
        "        cross_val_score(\n",
        "            gbm,\n",
        "            X_train,        # Training features\n",
        "            y_train,        # Training labels\n",
        "            cv=3,           # 3-fold cross-validation\n",
        "            n_jobs=-4,      # Parallel processing with 4 jobs\n",
        "            scoring='accuracy')  # Scoring based on accuracy\n",
        "    )\n",
        "\n",
        "    # Step 4: Negate the value since the optimizer minimizes the objective\n",
        "    # Return the negative of the accuracy as the objective function minimizes the value\n",
        "    return -value"
      ],
      "metadata": {
        "id": "4fNWtS2gyPO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Randomized Search\n"
      ],
      "metadata": {
        "id": "SGAYg78VzFXU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary functions and libraries for optimization\n",
        "from skopt import dummy_minimize  # for the randomized search\n",
        "\n",
        "# for the analysis of results after the search\n",
        "from skopt.plots import (\n",
        "    plot_convergence,  # plots the convergence of the search\n",
        "    plot_evaluations,  # plots the evaluations of the objective function\n",
        ")\n",
        "\n",
        "# Importing the hyperparameter space definition utilities\n",
        "from skopt.space import Real, Integer, Categorical\n",
        "from skopt.utils import use_named_args  # for mapping hyperparameters to functions\n",
        "\n",
        "# Step 1: Perform a random search over the hyperparameter space\n",
        "search = dummy_minimize(\n",
        "    objective,  # the objective function to minimize\n",
        "    param_grid,  # the hyperparameter space\n",
        "    n_calls=50,  # number of evaluations of the objective function\n",
        "    random_state=0,  # ensures reproducibility\n",
        ")"
      ],
      "metadata": {
        "id": "vwz8NxqmzF1M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plotting\n",
        "\n",
        "The evaluation plot shows how different choices of hyperparameters affect the model's performance. Hereâ€™s what it highlights:\n",
        "\n",
        "* Axes: Each axis represents a hyperparameter (like n_estimators or max_depth).\n",
        "\n",
        "* Points: Each point shows a specific combination of hyperparameters and how well that combination performed. Points that are darker or bigger usually mean better performance.\n",
        "\n",
        "* Trends: You can see which hyperparameter values tend to work better based on where the good-performing points are clustered.\n",
        "\n",
        "* Outliers: It helps you spot any unusual results where the performance is much worse or better than the others."
      ],
      "metadata": {
        "id": "gOFXKAbW0nY8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Plot the convergence of the search\n",
        "plot_convergence(search)  # Shows how the objective function value changes over time\n",
        "\n",
        "# Step 3: Plot evaluations for each dimension of the hyperparameter space\n",
        "dim_names = ['n_estimators', 'max_depth', 'min_samples_split', 'learning_rate', 'loss']  # hyperparameter names\n",
        "plot_evaluations(result=search, plot_dims=dim_names)  # Evaluates each hyperparameter's impact\n",
        "\n",
        "plt.show()  # Display the plots"
      ],
      "metadata": {
        "id": "XHfEBiDA0opf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bayesian Optimization"
      ],
      "metadata": {
        "id": "lHAba_pG2VtD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bayesian Optimization with Gaussian Process\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "argotMv92gHW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from skopt import gp_minimize # Bayesian Opt with GP\n",
        "\n",
        "# for the analysis\n",
        "from skopt.plots import (\n",
        "    plot_convergence,\n",
        "    plot_evaluations,\n",
        "    plot_objective,\n",
        ")\n",
        "\n",
        "from skopt.space import Real, Integer, Categorical\n",
        "from skopt.utils import use_named_args\n",
        "\n",
        "# gp_minimize performs by default GP Optimization\n",
        "# using a Marten Kernel\n",
        "\n",
        "gp_ = gp_minimize(\n",
        "    objective, # the objective function to minimize\n",
        "    param_grid, # the hyperparameter space\n",
        "    n_initial_points=10, # the number of points to evaluate f(x) to start of\n",
        "    acq_func='EI', # the acquisition function\n",
        "    n_calls=30, # the number of subsequent evaluations of f(x)\n",
        "    random_state=0,\n",
        ")"
      ],
      "metadata": {
        "id": "ecesKUTn2Xis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plotting"
      ],
      "metadata": {
        "id": "syQIo1_A22Ki"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Plot the convergence of the Bayesian optimization\n",
        "# Shows how the objective function value improves over iterations\n",
        "plot_convergence(gp_)\n",
        "\n",
        "# Step 2: Define the hyperparameter names for the objective and evaluation plots\n",
        "# Hyperparameter names\n",
        "dim_names = ['n_estimators', 'max_depth', 'min_samples_split', 'learning_rate', 'loss']\n",
        "\n",
        "# Step 3: Plot the objective for each hyperparameter\n",
        "# Shows the effect of each hyperparameter on the model's performance\n",
        "plot_objective(result=gp_, plot_dims=dim_names)\n",
        "plt.show()\n",
        "\n",
        "# Step 4: Plot the evaluations for each dimension of the hyperparameter space\n",
        "# Visualizes which hyperparameter values were evaluated\n",
        "plot_evaluations(result=gp_, plot_dims=dim_names)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "u8V027ow23al"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bayesian Optimization with Random Forests (SMAC)\n"
      ],
      "metadata": {
        "id": "bJQKRINQ9O-_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import the necessary modules for Bayesian optimization and analysis\n",
        "# skopt contains optimization functions and plotting tools\n",
        "from skopt import forest_minimize  # Bayesian Optimization with RF as surrogate\n",
        "from skopt.plots import plot_convergence, plot_evaluations, plot_objective  # Plotting tools for analysis\n",
        "from skopt.space import Real, Integer, Categorical  # Define the hyperparameter search space\n",
        "from skopt.utils import use_named_args  # Decorator to pass hyperparameters as named arguments\n",
        "\n",
        "# Step 2: Perform Bayesian optimization using Random Forest as the surrogate model\n",
        "# forest_minimize optimizes the objective function by exploring the hyperparameter space\n",
        "fm_ = forest_minimize(\n",
        "    objective,  # The objective function to minimize\n",
        "    param_grid,  # The hyperparameter space\n",
        "    base_estimator='RF',  # Use Random Forest as the surrogate model\n",
        "    n_initial_points=10,  # Number of points to evaluate the objective function to start with\n",
        "    acq_func='EI',  # Expected Improvement acquisition function\n",
        "    n_calls=30,  # Number of iterations for evaluating the objective function\n",
        "    random_state=0,  # Ensure reproducibility\n",
        "    n_jobs=4,  # Use 4 cores for parallel computation\n",
        ")"
      ],
      "metadata": {
        "id": "vPVl3vde9PdW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bayesian Optimization with GBM as surrogate\n"
      ],
      "metadata": {
        "id": "3G0ubSOj9baH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import the necessary modules for Bayesian optimization and analysis\n",
        "# skopt contains optimization functions and plotting tools\n",
        "from skopt import gbrt_minimize  # Bayesian Optimization with GBM as surrogate\n",
        "from skopt.plots import plot_convergence, plot_evaluations, plot_objective  # Plotting tools for analysis\n",
        "from skopt.space import Real, Integer, Categorical  # Define the hyperparameter search space\n",
        "from skopt.utils import use_named_args  # Decorator to pass hyperparameters as named arguments\n",
        "\n",
        "# Step 2: Perform Bayesian optimization using Gradient Boosted Machines as the surrogate\n",
        "# gbrt_minimize optimizes the objective function by exploring the hyperparameter space\n",
        "gbm_ = gbrt_minimize(\n",
        "    objective,  # The objective function to minimize\n",
        "    param_grid,  # The hyperparameter space\n",
        "    n_initial_points=10,  # Number of points to evaluate the objective function to start with\n",
        "    acq_func='EI',  # Expected Improvement acquisition function\n",
        "    n_calls=30,  # Number of iterations for evaluating the objective function\n",
        "    random_state=0,  # Ensure reproducibility\n",
        "    n_jobs=4,  # Use 4 cores for parallel computation\n",
        ")"
      ],
      "metadata": {
        "id": "CfSLjFaD9b6l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note**\n",
        "* **RF** is better for noisy, irregular problems, where you need a more exploration-driven search.\n",
        "* **GBM** is ideal when faster convergence and higher precision are desired, though it may require more computational resources."
      ],
      "metadata": {
        "id": "DNxmO3npPHFp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bayesian Optimization with XGBoost"
      ],
      "metadata": {
        "id": "kR3EvXblWzTJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import XGBoost and necessary libraries for optimization\n",
        "import xgboost as xgb  # Importing the XGBoost library for model building\n",
        "from skopt import gp_minimize  # Importing the Gaussian Process optimization function\n",
        "\n",
        "# Step 2: Import plotting functions for analysis\n",
        "# These functions will help visualize the optimization process\n",
        "from skopt.plots import (\n",
        "    plot_convergence,  # For plotting the convergence of the optimization\n",
        "    plot_evaluations,  # For visualizing the evaluation of hyperparameters\n",
        "    plot_objective,    # For plotting the objective function values\n",
        ")\n",
        "from skopt.space import Real, Integer, Categorical  # For defining hyperparameter space\n",
        "from skopt.utils import use_named_args  # For using named arguments in the objective function\n",
        "\n",
        "# Step 3: Set up the Gaussian Process Optimization\n",
        "# gp_minimize performs Bayesian Optimization using a Marten Kernel by default\n",
        "gp_ = gp_minimize(\n",
        "    objective,  # The objective function to minimize\n",
        "    param_grid,  # The hyperparameter space to explore\n",
        "    n_initial_points=10,  # Number of initial points to evaluate f(x)\n",
        "    acq_func='EI',  # The acquisition function used for optimization\n",
        "    n_calls=40,  # Number of subsequent evaluations of f(x)\n",
        "    random_state=0,  # Ensures reproducibility of results\n",
        ")"
      ],
      "metadata": {
        "id": "ThVCpHA4Wzi7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parallelization Optimization with Gaussian Process\n"
      ],
      "metadata": {
        "id": "V_xCEfGDN-kD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import necessary modules for optimization and parallelization\n",
        "# Optimizer is used for the optimization process, Parallel and delayed for parallel computations\n",
        "from skopt import Optimizer  # For the optimization process\n",
        "from joblib import Parallel, delayed  # For parallel computation\n",
        "\n",
        "# Step 2: Initialize the Optimizer for Bayesian Optimization\n",
        "# Optimizer uses Gaussian Processes (GP) as the surrogate model\n",
        "optimizer = Optimizer(\n",
        "    dimensions=param_grid,  # The hyperparameter space\n",
        "    base_estimator=\"GP\",  # The surrogate model (Gaussian Process)\n",
        "    n_initial_points=10,  # Number of initial points to evaluate the objective function\n",
        "    acq_func='EI',  # Expected Improvement acquisition function\n",
        "    random_state=0,  # Ensure reproducibility\n",
        "    n_jobs=4,  # Number of cores for parallel computation\n",
        ")\n",
        "\n",
        "# Step 3: Perform optimization in parallel over 10 iterations\n",
        "# Each iteration evaluates 4 points in parallel and updates the optimizer\n",
        "for i in range(10):\n",
        "    x = optimizer.ask(n_points=4)  # Generate 4 points to evaluate the objective function\n",
        "    y = Parallel(n_jobs=4)(delayed(objective)(v) for v in x)  # Evaluate the objective function in parallel\n",
        "    optimizer.tell(x, y)  # Update the optimizer with the evaluated points"
      ],
      "metadata": {
        "id": "0P9lNfe4OEIt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note**\n",
        "\n",
        "Bayesian optimization itself is not inherently a parallel algorithm, as it traditionally works sequentially. Each new set of hyperparameters is chosen based on the information from all the previous evaluations. However, it can be adapted for parallelization using certain strategies.\n",
        "\n",
        "**Important**\n",
        "\n",
        "When you run the optimization in parallel, the algorithm picks multiple sets of hyperparameters at the same time (e.g., 4 sets) and evaluates them simultaneously. The trade-off here is that while this parallel evaluation speeds up the process, the algorithm doesn't get the chance to learn from each individual evaluation before choosing the next set of hyperparameters.\n",
        "\n"
      ],
      "metadata": {
        "id": "TlcatJEbOmjF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import BayesSearchCV for hyperparameter optimization\n",
        "# BayesSearchCV is a wrapper for using Bayesian optimization in hyperparameter tuning\n",
        "from skopt import BayesSearchCV\n",
        "\n",
        "# Step 2: Set up the BayesSearchCV for hyperparameter search\n",
        "# This initializes the search with the specified estimator and parameter grid\n",
        "search = BayesSearchCV(\n",
        "    estimator=gbm,  # The model to optimize\n",
        "    search_spaces=param_grid,  # The hyperparameter space to explore\n",
        "    scoring='neg_mean_squared_error',  # Metric for evaluation (negated for minimization)\n",
        "    cv=3,  # Number of cross-validation folds\n",
        "    n_iter=50,  # Number of iterations for the search\n",
        "    random_state=10,  # Ensures reproducibility of results\n",
        "    n_jobs=4,  # Number of CPU cores for parallel processing\n",
        "    refit=True  # Refits the model using the best found hyperparameters\n",
        ")\n",
        "\n",
        "# Step 3: Fit the BayesSearchCV to the training data\n",
        "# This will start the hyperparameter optimization process\n",
        "search.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "34obgZmlPS1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bayesian Optimisation with different Kernels\n"
      ],
      "metadata": {
        "id": "NrDqt7qjRKb9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import the squared exponential kernel and necessary libraries\n",
        "# Importing Radial Basis Function (RBF) for Gaussian Process Regression\n",
        "from sklearn.gaussian_process.kernels import RBF\n",
        "from skopt import gp_minimize  # For performing Bayesian Optimization\n",
        "from skopt.plots import plot_convergence  # For plotting the convergence of the optimization process\n",
        "from skopt.space import Real, Integer, Categorical  # For defining the hyperparameter space\n",
        "from skopt.utils import use_named_args  # For using named arguments in the objective function\n",
        "from skopt.learning import GaussianProcessRegressor  # Importing the Gaussian Process Regressor\n",
        "\n",
        "# Step 2: Define the kernel for Gaussian Process Regression\n",
        "# This kernel is a Radial Basis Function (RBF) with specified length scale and bounds\n",
        "kernel = 1.0 * RBF(length_scale=1.0, length_scale_bounds=(1e-1, 10.0))\n",
        "\n",
        "# Display the kernel\n",
        "kernel  # This line shows the defined kernel for confirmation\n",
        "\n",
        "# Step 3: Initialize the Gaussian Process Regressor\n",
        "# This sets up the regressor with the defined kernel and additional parameters\n",
        "gpr = GaussianProcessRegressor(\n",
        "    kernel=kernel,  # The kernel defined above\n",
        "    normalize_y=True,  # Normalize the output for better stability\n",
        "    noise=\"gaussian\",  # Assumes Gaussian noise in the observations\n",
        "    n_restarts_optimizer=2  # Number of restarts for the optimizer\n",
        ")\n",
        "\n",
        "# Step 4: Perform Bayesian Optimization using gp_minimize\n",
        "# This function optimizes the objective function using the Gaussian Process Regressor as the surrogate model\n",
        "gp_ = gp_minimize(\n",
        "    objective,  # The objective function to minimize\n",
        "    dimensions=param_grid,  # The hyperparameter space to explore\n",
        "    base_estimator=gpr,  # The Gaussian Process Regressor as the surrogate model\n",
        "    n_initial_points=5,  # Number of initial points to evaluate\n",
        "    acq_optimizer=\"sampling\",  # Method for acquisition function optimization\n",
        "    random_state=42  # Ensures reproducibility of results\n",
        ")"
      ],
      "metadata": {
        "id": "QI9mTLzURWCF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Description**\n",
        "\n",
        "You define the GaussianProcessRegressor whenever you want to use a different kernel. Itâ€™s a common approach to ensure that your model leverages the specific properties of the kernel you are interested in.\n",
        "The GaussianProcessRegressor serves as the interface that controls how the kernel interacts with your data, helping to refine the predictions based on the underlying assumptions of that kernel."
      ],
      "metadata": {
        "id": "JtLGjIEAalU8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Side node**\n",
        "\n",
        "Use GaussianProcessRegressor when your task is to predict continuous values (regression). For example, predicting house prices or temperature based on input features.\n",
        "\n",
        "Use GaussianProcessClassifier when your task is to categorize data into distinct classes (classification). For example, determining if an email is spam or classifying images into different categories."
      ],
      "metadata": {
        "id": "yWBm_muTcXG8"
      }
    }
  ]
}