{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Parameters vs Hyperparameters"
      ],
      "metadata": {
        "id": "QZQ2NUqkLojw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Parameters** are the internal values that a model learns from the training data. These values adjust automatically during the learning process to help the model make accurate predictions. For example, in linear regression, parameters are the coefficients that define the slope of the line, and in neural networks, they are the weights and biases that determine how data is passed through layers. Parameters are the model's way of adapting to the data and are updated during training.\n",
        "\n",
        "**Hyperparameters** are settings that are defined before the model begins training, and they control the learning process itself. These are not learned from data but are manually set by the user to influence how the model works. Examples of hyperparameters include the number of trees in a random forest or the learning rate in a neural network. They remain fixed during training unless manually tuned for better performance."
      ],
      "metadata": {
        "id": "jY-36qEUJrGL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Metrics"
      ],
      "metadata": {
        "id": "7phniAZlLUT0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classification"
      ],
      "metadata": {
        "id": "nN3CogehLWIE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dependent on the Probability Threshold:**\n",
        "Metrics like accuracy, precision, recall, F1 score, false positive rate (FPR), and false negative rate (FNR) rely on a specific probability threshold. This threshold decides whether a prediction is classified as positive or negative. Changing the threshold (e.g., 0.5, 0.7) affects how many predictions are classified as each class, which in turn alters these metrics.\n",
        "\n",
        "* Accuracy: Measures the percentage of correct predictions at a chosen threshold.\n",
        "* Precision: Measures the proportion of positive predictions that are correct.\n",
        "* Recall: Measures how many actual positives were correctly identified.\n",
        "* F1 Score: Balances precision and recall.\n",
        "* FPR/FNR: Measure how often the model incorrectly classifies negatives or positives, respectively.\n",
        "\n",
        "\n",
        "**Independent of the Probability Threshold:**\n",
        "ROC-AUC (Receiver Operating Characteristic - Area Under the Curve) is independent of any specific threshold. It evaluates the model's ability to distinguish between classes across all possible thresholds, providing a more holistic view of how well the model separates positives and negatives."
      ],
      "metadata": {
        "id": "56B5IFmNMATM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regression"
      ],
      "metadata": {
        "id": "TU6EqzOz33_L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Mean Squared Error (MSE)\n",
        "Measures how far the predicted values are from the actual values, with a focus on larger errors. Lower MSE is better.\n",
        "* Root Mean Squared Error (RMSE)\n",
        "Similar to MSE but in the same units as the original data, making it easier to interpret. Lower RMSE indicates better performance.\n",
        "* Mean Absolute Error (MAE)\n",
        "Averages the absolute differences between predicted and actual values. It treats all errors equally and is more resistant to outliers.\n",
        "* R-squared (R²)\n",
        "Tells how well the model explains the variation in the data. A value closer to 1 means the model fits the data well.\n",
        "\n",
        "**Side notes**\n",
        "* **Square:** Highlights bigger errors.\n",
        "* **Root:** Makes the numbers easier to understand.\n",
        "* **Absolute:** Treats all errors fairly, focusing on how far off we are in general."
      ],
      "metadata": {
        "id": "wh2iZOXd36Dj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cross validation"
      ],
      "metadata": {
        "id": "F5O2yKSV4WoQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**K-Fold cross-validation** involves splitting the dataset into k equal parts (called folds). The model is trained on k-1 folds and tested on the remaining fold. This process is repeated k times, with each fold used once as the test set. It’s commonly used for general model performance evaluation when you have enough data. The assumption here is that data is randomly and evenly distributed across the folds. When k is small, the model may not get enough training data, which can result in unreliable estimates.\n",
        "\n",
        "**Leave-One-Out (LOO)** cross-validation is a special case of K-Fold where k equals the number of samples. Each iteration trains the model on all but one data point and tests on that single point. It’s used when working with very small datasets, where every individual data point is crucial. The assumption is that all data points are independent. Since this method uses almost all data for training in each iteration, it can be computationally intensive but provides an unbiased estimate of performance.\n",
        "\n",
        "**Leave-P-Out (LPO)** cross-validation is similar to LOO, but instead of leaving out just one data point, p points are left out for testing in each iteration. This is useful for small datasets but can be computationally expensive as the number of combinations increases rapidly when p grows. The assumption remains that the data is evenly distributed, and the technique provides detailed model evaluation when p is small, though it can be inefficient for large datasets.\n",
        "* it ensures that each unique combination of 2 data points is used exactly once for testing, and the remaining 98 are used for training.\n",
        "* Every data point will eventually be part of the test set across different iterations, but the same pair of data points won’t be repeated in a test set.\n",
        "\n",
        "**Repeated K-Fold** cross-validation takes the K-Fold approach and repeats it multiple times with different random splits of the data. This helps average out randomness and provides a more robust estimate of model performance. The assumption is similar to K-Fold: the data is randomly distributed, but by repeating the process, any variance due to random splits is smoothed out. This method is particularly helpful when k is small, as multiple repetitions can reduce the bias of smaller folds.\n",
        "\n",
        "**Stratified K-Fold** cross-validation ensures that each fold has the same class distribution as the original dataset, which is especially important for classification problems with imbalanced classes. It’s used when dealing with classification tasks, where you want to maintain the proportion of each class across training and test sets. The assumption is that the dataset must have enough samples in each class to preserve balance. When k is small, class imbalance might distort the representation within each fold, so this method helps ensure balance."
      ],
      "metadata": {
        "id": "hhNRRYF64Ye1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from sklearn.model_selection import (\n",
        "    KFold,\n",
        "    RepeatedKFold,\n",
        "    LeaveOneOut,\n",
        "    LeavePOut,\n",
        "    StratifiedKFold,\n",
        "    cross_validate,\n",
        "    train_test_split,\n",
        ")"
      ],
      "metadata": {
        "id": "BaLexnwM4JGk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### K-Fold Cross-Validation\n"
      ],
      "metadata": {
        "id": "1AtWp7pr6zAI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Create a Logistic Regression model\n",
        "# Logistic Regression with L2 regularization, C=10, using 'liblinear' solver, fixed random state, and max iterations set to 10000\n",
        "logit = LogisticRegression(\n",
        "    penalty='l2', C=10, solver='liblinear', random_state=4, max_iter=10000)\n",
        "\n",
        "# Step 2: Set up K-Fold Cross-Validation\n",
        "# Creating a K-Fold object with 5 splits, shuffling the data, and setting a random state for reproducibility\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=4)\n",
        "\n",
        "# Step 3: Estimate generalization error\n",
        "# Using cross-validation to evaluate the model's performance on training and test data\n",
        "clf = cross_validate(\n",
        "    logit,\n",
        "    X_train,  # Features for training\n",
        "    y_train,  # Target variable for training\n",
        "    scoring='accuracy',  # Metric for evaluation\n",
        "    return_train_score=True,  # Include training scores in the output\n",
        "    cv=kf,  # Use K-Fold cross-validation\n",
        ")\n",
        "\n",
        "# Step 4: Retrieve test scores\n",
        "# Accessing the test scores from the cross-validation results\n",
        "clf['test_score']\n",
        "\n",
        "# Step 5: Retrieve training scores\n",
        "# Accessing the training scores from the cross-validation results\n",
        "clf['train_score']\n",
        "\n",
        "# Step 6: Print mean train set accuracy\n",
        "# Calculating and printing the mean accuracy and standard deviation for the training set\n",
        "print('mean train set accuracy: ', np.mean(clf['train_score']), ' +- ', np.std(clf['train_score']))\n",
        "\n",
        "# Step 7: Print mean test set accuracy\n",
        "# Calculating and printing the mean accuracy and standard deviation for the test set\n",
        "print('mean test set accuracy: ', np.mean(clf['test_score']), ' +- ', np.std(clf['test_score']))"
      ],
      "metadata": {
        "id": "zb1XTDrxLrqF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Repeated K-Fold\n"
      ],
      "metadata": {
        "id": "aLldrWpz7KAf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Create a Logistic Regression model\n",
        "# Logistic Regression with L2 regularization, C=1, using 'liblinear' solver, fixed random state, and max iterations set to 10000\n",
        "logit = LogisticRegression(\n",
        "    penalty='l2', C=1, solver='liblinear', random_state=4, max_iter=10000)\n",
        "\n",
        "# Step 2: Set up Repeated K-Fold Cross-Validation\n",
        "# Creating a Repeated K-Fold object with 5 splits, repeating 10 times, and a random state for reproducibility\n",
        "rkf = RepeatedKFold(\n",
        "    n_splits=5,  # Number of folds\n",
        "    n_repeats=10,  # Number of repetitions\n",
        "    random_state=4,  # Seed for reproducibility\n",
        ")\n",
        "\n",
        "# Step 3: Print expected number of performance metrics\n",
        "# Calculating and displaying the expected number of performance metrics (5 folds * 10 repeats)\n",
        "print('We expect K * n performance metrics: ', 5 * 10)\n",
        "\n",
        "# Step 4: Estimate generalization error\n",
        "# Using cross-validation to evaluate the model's performance on training and test data\n",
        "clf = cross_validate(\n",
        "    logit,\n",
        "    X_train,  # Features for training\n",
        "    y_train,  # Target variable for training\n",
        "    scoring='accuracy',  # Metric for evaluation\n",
        "    return_train_score=True,  # Include training scores in the output\n",
        "    cv=rkf,  # Use Repeated K-Fold cross-validation\n",
        ")\n",
        "\n",
        "# Step 5: Print the number of metrics obtained\n",
        "# Displaying the total number of test scores obtained from cross-validation\n",
        "print('Number of metrics obtained: ', len(clf['test_score']))\n",
        "\n",
        "# Step 6: Access test scores\n",
        "# Accessing the test scores from the cross-validation results\n",
        "clf['test_score']\n",
        "\n",
        "# Step 7: Print mean train set accuracy\n",
        "# Calculating and printing the mean accuracy and standard deviation for the training set\n",
        "print('mean train set accuracy: ', np.mean(clf['train_score']), ' +- ', np.std(clf['train_score']))\n",
        "\n",
        "# Step 8: Print mean test set accuracy\n",
        "# Calculating and printing the mean accuracy and standard deviation for the test set\n",
        "print('mean test set accuracy: ', np.mean(clf['test_score']), ' +- ', np.std(clf['test_score']))"
      ],
      "metadata": {
        "id": "3ygfUg687Kl1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Leave One Out\n"
      ],
      "metadata": {
        "id": "ks2F9T9N7QG3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Create a Logistic Regression model\n",
        "# Logistic Regression with L2 regularization, C=1, using 'liblinear' solver, fixed random state, and max iterations set to 10000\n",
        "logit = LogisticRegression(\n",
        "    penalty='l2', C=1, solver='liblinear', random_state=4, max_iter=10000)\n",
        "\n",
        "# Step 2: Set up Leave One Out Cross-Validation\n",
        "# Creating a Leave One Out cross-validation object for evaluating the model\n",
        "loo = LeaveOneOut()\n",
        "\n",
        "# Step 3: Print expected number of metrics\n",
        "# Displaying the expected number of metrics, which is equal to the number of samples in the training set\n",
        "print('We expect as many metrics as data in the train set: ', len(X_train))\n",
        "\n",
        "# Step 4: Estimate generalization error\n",
        "# Using cross-validation to evaluate the model's performance on training and test data\n",
        "clf = cross_validate(\n",
        "    logit,\n",
        "    X_train,  # Features for training\n",
        "    y_train,  # Target variable for training\n",
        "    scoring='accuracy',  # Metric for evaluation\n",
        "    return_train_score=True,  # Include training scores in the output\n",
        "    cv=loo,  # Use Leave One Out cross-validation\n",
        ")\n",
        "\n",
        "# Step 5: Print the number of metrics obtained\n",
        "# Displaying the total number of test scores obtained from cross-validation\n",
        "print('Number of metrics obtained: ', len(clf['test_score']))\n",
        "\n",
        "# Step 6: Access test scores\n",
        "# Accessing the test scores from the cross-validation results\n",
        "len(clf['test_score'])\n",
        "\n",
        "# Step 7: Print mean train set accuracy\n",
        "# Calculating and printing the mean accuracy and standard deviation for the training set\n",
        "print('mean train set accuracy: ', np.mean(clf['train_score']), ' +- ', np.std(clf['train_score']))\n",
        "\n",
        "# Step 8: Print mean test set accuracy\n",
        "# Calculating and printing the mean accuracy and standard deviation for the test set\n",
        "print('mean test set accuracy: ', np.mean(clf['test_score']), ' +- ', np.std(clf['test_score']))"
      ],
      "metadata": {
        "id": "hdew7aXn7QkI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Leave P Out"
      ],
      "metadata": {
        "id": "3qrL1PEe7UQf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Create a Logistic Regression model\n",
        "# Logistic Regression with L2 regularization, C=1, using 'liblinear' solver, fixed random state, and max iterations set to 10000\n",
        "logit = LogisticRegression(\n",
        "    penalty='l2', C=1, solver='liblinear', random_state=4, max_iter=10000)\n",
        "\n",
        "# Step 2: Set up Leave P Out Cross-Validation\n",
        "# Creating a Leave P Out cross-validation object where p=2, meaning 2 samples will be left out for testing\n",
        "lpo = LeavePOut(p=2)\n",
        "\n",
        "# Step 3: Take a smaller sample of the data\n",
        "# Selecting a smaller sample of 100 data points to avoid memory issues during computation\n",
        "X_train_small = X_train.head(100)\n",
        "y_train_small = y_train.head(100)\n",
        "\n",
        "# Step 4: Calculate expected number of metrics\n",
        "# Calculating and printing the expected number of metrics based on combinations of 100 data points taken 2 at a time\n",
        "print('We expect : ', comb(100, 2), ' metrics')\n",
        "\n",
        "# Step 5: Estimate generalization error\n",
        "# Using cross-validation to evaluate the model's performance on the small sample of training data\n",
        "clf = cross_validate(\n",
        "    logit,\n",
        "    X_train_small,  # Features for training\n",
        "    y_train_small,  # Target variable for training\n",
        "    scoring='accuracy',  # Metric for evaluation\n",
        "    return_train_score=True,  # Include training scores in the output\n",
        "    cv=lpo,  # Use Leave P Out cross-validation\n",
        ")\n",
        "\n",
        "# Step 6: Print the number of metrics obtained\n",
        "# Displaying the total number of test scores obtained from cross-validation\n",
        "print('Number of metrics obtained: ', len(clf['test_score']))\n",
        "\n",
        "# Step 7: Print mean train set accuracy\n",
        "# Calculating and printing the mean accuracy and standard deviation for the training set\n",
        "print('mean train set accuracy: ', np.mean(clf['train_score']), ' +- ', np.std(clf['train_score']))\n",
        "\n",
        "# Step 8: Print mean test set accuracy\n",
        "# Calculating and printing the mean accuracy and standard deviation for the test set\n",
        "print('mean test set accuracy: ', np.mean(clf['test_score']), ' +- ', np.std(clf['test_score']))"
      ],
      "metadata": {
        "id": "-kZPHs617WvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stratified K-Fold Cross-Validation\n"
      ],
      "metadata": {
        "id": "xuHOZE-o7ZF-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Create a Logistic Regression model\n",
        "# Logistic Regression with L2 regularization, C=1, using 'liblinear' solver, fixed random state, and max iterations set to 10000\n",
        "logit = LogisticRegression(\n",
        "    penalty='l2', C=1, solver='liblinear', random_state=4, max_iter=10000)\n",
        "\n",
        "# Step 2: Set up Stratified K-Fold Cross-Validation\n",
        "# Creating a Stratified K-Fold cross-validation object with 5 splits, shuffling data, and a fixed random state\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=4)\n",
        "\n",
        "# Step 3: Estimate generalization error\n",
        "# Using cross-validation to evaluate the model's performance on the training data\n",
        "clf = cross_validate(\n",
        "    logit,\n",
        "    X_train,  # Features for training\n",
        "    y_train,  # Target variable for training\n",
        "    scoring='accuracy',  # Metric for evaluation\n",
        "    return_train_score=True,  # Include training scores in the output\n",
        "    cv=skf,  # Use Stratified K-Fold cross-validation\n",
        ")\n",
        "\n",
        "# Step 4: Print the number of metrics obtained\n",
        "# Displaying the total number of test scores obtained from cross-validation\n",
        "len(clf['test_score'])\n",
        "\n",
        "# Step 5: Print mean train set accuracy\n",
        "# Calculating and printing the mean accuracy and standard deviation for the training set\n",
        "print('mean train set accuracy: ', np.mean(clf['train_score']), ' +- ', np.std(clf['train_score']))\n",
        "\n",
        "# Step 6: Print mean test set accuracy\n",
        "# Calculating and printing the mean accuracy and standard deviation for the test set\n",
        "print('mean test set accuracy: ', np.mean(clf['test_score']), ' +- ', np.std(clf['test_score']))"
      ],
      "metadata": {
        "id": "FheLrrnr7Z8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cross validation for hyperparameters"
      ],
      "metadata": {
        "id": "8OYy5MMuDmEO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## K-Fold Cross-Validation"
      ],
      "metadata": {
        "id": "3toAQRwzLYrz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from sklearn.model_selection import (\n",
        "    KFold,\n",
        "    RepeatedKFold,\n",
        "    LeaveOneOut,\n",
        "    LeavePOut,\n",
        "    StratifiedKFold,\n",
        "    GridSearchCV,\n",
        "    train_test_split,\n",
        ")"
      ],
      "metadata": {
        "id": "_3QiOSm61tyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Logistic Regression model definition\n",
        "# Initializes a logistic regression model with L2 regularization and specified settings\n",
        "logit = LogisticRegression(\n",
        "    penalty ='l2',        # Regularization type: L2\n",
        "    C=1,                  # Inverse of regularization strength\n",
        "    solver='liblinear',   # Optimization algorithm used\n",
        "    random_state=4,       # Seed for reproducibility\n",
        "    max_iter=10000        # Maximum number of iterations for convergence\n",
        ")\n",
        "\n",
        "# Step 2: Define hyperparameter space for tuning\n",
        "# Creates a dictionary of hyperparameters to search through during Grid Search\n",
        "param_grid = dict(\n",
        "    penalty=['l1', 'l2'],  # Regularization options to explore\n",
        "    C=[0.1, 1, 10],        # Values for inverse regularization strength to explore\n",
        ")\n",
        "\n",
        "# Step 3: Set up K-Fold Cross-Validation\n",
        "# Sets up 5-fold cross-validation with shuffling to create training/validation splits\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=4)  # 5-fold CV with shuffling\n",
        "\n",
        "# Step 4: Set up Grid Search for hyperparameter tuning\n",
        "# Configures Grid Search to tune hyperparameters using accuracy and cross-validation\n",
        "clf =  GridSearchCV(\n",
        "    logit,                  # Model to tune\n",
        "    param_grid,            # Hyperparameter space to explore\n",
        "    scoring='accuracy',     # Metric to optimize\n",
        "    cv=kf,                  # Cross-validation strategy\n",
        "    refit=True,            # Refits best model to the entire dataset\n",
        ")\n",
        "\n",
        "# Step 5: Fit Grid Search to training data\n",
        "# Trains the model using Grid Search with the training data\n",
        "search = clf.fit(X_train, y_train)\n",
        "\n",
        "# Step 6: Retrieve best hyperparameters found\n",
        "# Returns the optimal hyperparameters found during Grid Search\n",
        "search.best_params_\n",
        "\n",
        "# Step 7: Create a DataFrame with the results of the grid search\n",
        "# Converts cross-validation results into a DataFrame for analysis\n",
        "results = pd.DataFrame(search.cv_results_)[['params', 'mean_test_score', 'std_test_score']]\n",
        "print(results.shape)  # Display shape of results DataFrame\n",
        "results  # Show results DataFrame\n",
        "\n",
        "# Step 8: Sort results by mean test score in descending order\n",
        "# Sorts the DataFrame by mean test score to find the best performing hyperparameters\n",
        "results.sort_values(by='mean_test_score', ascending=False, inplace=True)\n",
        "\n",
        "# Step 9: Reset index of the results DataFrame\n",
        "# Resets the index of the DataFrame after sorting for better readability\n",
        "results.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Step 10: Plot mean test scores with error bars\n",
        "# Visualizes the mean test scores with error bars showing standard deviation\n",
        "results['mean_test_score'].plot(\n",
        "    yerr=[results['std_test_score'], results['std_test_score']],  # Error bars for standard deviation\n",
        "    subplots=True            # Create subplots for better visual clarity\n",
        ")\n",
        "\n",
        "# Step 11: Label the y-axis\n",
        "# Adds a label to the y-axis of the plot indicating it represents accuracy\n",
        "plt.ylabel('Mean Accuracy')\n",
        "\n",
        "# Step 12: Label the x-axis\n",
        "# Adds a label to the x-axis of the plot indicating it represents hyperparameter space\n",
        "plt.xlabel('Hyperparameter space')"
      ],
      "metadata": {
        "id": "TE0bu7f8DqXv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Repeated K-Fold"
      ],
      "metadata": {
        "id": "bQrx0PiS2R7r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic Regression\n",
        "# Defines a logistic regression model with L2 regularization and specific configurations\n",
        "logit = LogisticRegression(\n",
        "    penalty ='l2',         # Regularization type: L2\n",
        "    C=1,                   # Inverse of regularization strength\n",
        "    solver='liblinear',    # Optimization algorithm used\n",
        "    random_state=4,        # Seed for reproducibility\n",
        "    max_iter=10000         # Maximum number of iterations for convergence\n",
        ")\n",
        "\n",
        "# hyperparameter space\n",
        "# Creates a dictionary of hyperparameters for tuning the model\n",
        "param_grid = dict(\n",
        "    penalty=['l1', 'l2'],  # Regularization options to explore\n",
        "    C=[0.1, 1, 10],        # Values for inverse regularization strength to explore\n",
        ")\n",
        "\n",
        "# Repeated K-Fold Cross-Validation\n",
        "# Sets up repeated K-Fold cross-validation with 5 splits and 10 repeats\n",
        "rkf = RepeatedKFold(\n",
        "    n_splits=5,           # Number of splits for K-Fold\n",
        "    n_repeats=10,         # Number of times to repeat the cross-validation\n",
        "    random_state=4,       # Seed for reproducibility\n",
        ")\n",
        "\n",
        "# search\n",
        "# Configures Grid Search with the model, hyperparameters, accuracy metric, and repeated K-Fold CV\n",
        "clf = GridSearchCV(\n",
        "    logit,                  # Model to tune\n",
        "    param_grid,            # Hyperparameter space to explore\n",
        "    scoring='accuracy',     # Metric to optimize\n",
        "    cv=rkf,                # Cross-validation strategy\n",
        "    refit=True,            # Refits best model to entire dataset\n",
        ")\n",
        "\n",
        "search = clf.fit(X_train, y_train)  # Fits the model to the training data using Grid Search\n",
        "\n",
        "# best hyperparameters\n",
        "# Retrieves the optimal hyperparameters from Grid Search\n",
        "search.best_params_\n",
        "\n",
        "# Creates a DataFrame from Grid Search results, showing hyperparameters, mean, and standard deviation of scores\n",
        "results = pd.DataFrame(search.cv_results_)[['params', 'mean_test_score', 'std_test_score']]\n",
        "print(results.shape)  # Displays the shape of the results DataFrame\n",
        "\n",
        "results  # Shows the results DataFrame\n",
        "\n",
        "# Sorts the DataFrame by mean test score in descending order\n",
        "results.sort_values(by='mean_test_score', ascending=False, inplace=True)\n",
        "\n",
        "# Resets the index of the DataFrame after sorting\n",
        "results.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Plots mean test scores with error bars indicating the standard deviation\n",
        "results['mean_test_score'].plot(yerr=[results['std_test_score'], results['std_test_score']], subplots=True)\n",
        "\n",
        "# Adds a label to the y-axis representing accuracy\n",
        "plt.ylabel('Mean Accuracy')\n",
        "\n",
        "# Adds a label to the x-axis representing hyperparameter space\n",
        "plt.xlabel('Hyperparameter space')\n",
        "\n",
        "# let's get the predictions\n",
        "# Generates predictions for the training and testing sets using the best model\n",
        "train_preds = search.predict(X_train)\n",
        "test_preds = search.predict(X_test)\n",
        "\n",
        "# Prints the training accuracy by comparing predictions to actual labels\n",
        "print('Train Accuracy: ', accuracy_score(y_train, train_preds))\n",
        "\n",
        "# Prints the testing accuracy by comparing predictions to actual labels\n",
        "print('Test Accuracy: ', accuracy_score(y_test, test_preds))"
      ],
      "metadata": {
        "id": "wo-hJSfb2TVm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stratified K-Fold Cross-Validation\n"
      ],
      "metadata": {
        "id": "22YWlKAB2j_D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic Regression\n",
        "# Defines a logistic regression model with L2 regularization and specific configurations\n",
        "logit = LogisticRegression(\n",
        "    penalty ='l2',         # Regularization type: L2\n",
        "    C=1,                   # Inverse of regularization strength\n",
        "    solver='liblinear',    # Optimization algorithm used\n",
        "    random_state=4,        # Seed for reproducibility\n",
        "    max_iter=10000         # Maximum number of iterations for convergence\n",
        ")\n",
        "\n",
        "# hyperparameter space\n",
        "# Creates a dictionary of hyperparameters for tuning the model\n",
        "param_grid = dict(\n",
        "    penalty=['l1', 'l2'],  # Regularization options to explore\n",
        "    C=[0.1, 1, 10],        # Values for inverse regularization strength to explore\n",
        ")\n",
        "\n",
        "# Stratified Cross-Validation\n",
        "# Sets up Stratified K-Fold cross-validation with 5 splits and shuffling for stratified sampling\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=4)  # Stratified K-Fold CV\n",
        "\n",
        "# search\n",
        "# Configures Grid Search with the model, hyperparameters, accuracy metric, and Stratified K-Fold CV\n",
        "clf = GridSearchCV(\n",
        "    logit,                  # Model to tune\n",
        "    param_grid,            # Hyperparameter space to explore\n",
        "    scoring='accuracy',     # Metric to optimize\n",
        "    cv=skf,                # Cross-validation strategy\n",
        "    refit=True,            # Refits best model to entire dataset\n",
        ")\n",
        "\n",
        "search = clf.fit(X_train, y_train)  # Fits the model to the training data using Grid Search\n",
        "\n",
        "# best hyperparameters\n",
        "# Retrieves the optimal hyperparameters from Grid Search\n",
        "search.best_params_\n",
        "\n",
        "# Creates a DataFrame from Grid Search results, showing hyperparameters, mean, and standard deviation of scores\n",
        "results = pd.DataFrame(search.cv_results_)[['params', 'mean_test_score', 'std_test_score']]  # Extract relevant results\n",
        "print(results.shape)  # Displays the shape of the results DataFrame\n",
        "\n",
        "results.head()  # Shows the first few rows of the results DataFrame\n",
        "\n",
        "# Sorts the DataFrame by mean test score in descending order\n",
        "results.sort_values(by='mean_test_score', ascending=False, inplace=True)\n",
        "\n",
        "# Resets the index of the DataFrame after sorting\n",
        "results.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Plots mean test scores with error bars indicating the standard deviation\n",
        "results['mean_test_score'].plot(yerr=[results['std_test_score'], results['std_test_score']], subplots=True)\n",
        "\n",
        "# Adds a label to the y-axis representing accuracy\n",
        "plt.ylabel('Mean Accuracy')\n",
        "\n",
        "# Adds a label to the x-axis representing hyperparameter space\n",
        "plt.xlabel('Hyperparameter space')\n",
        "\n",
        "# let's get the predictions\n",
        "# Generates predictions for the training and testing sets using the best model\n",
        "train_preds = search.predict(X_train)  # Predictions on training data\n",
        "test_preds = search.predict(X_test)      # Predictions on testing data\n",
        "\n",
        "# Prints the training accuracy by comparing predictions to actual labels\n",
        "print('Train Accuracy: ', accuracy_score(y_train, train_preds))\n",
        "\n",
        "# Prints the testing accuracy by comparing predictions to actual labels\n",
        "print('Test Accuracy: ', accuracy_score(y_test, test_preds))"
      ],
      "metadata": {
        "id": "NXNyb6Rh2kZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Group Cross Validation"
      ],
      "metadata": {
        "id": "uqAmFI888iSA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Group K-Fold Cross-Validation**\n",
        "* What it is: A variation of K-Fold cross-validation where the data is divided into groups (or clusters). Each group contains related observations, and the goal is to ensure that all observations from a group are either in the training set or the validation set, but not both.\n",
        "* When to use it: Use Group K-Fold when you have data that is grouped in some way (like patients in a hospital or students in a school), and you want to prevent data leakage. For example, you wouldn’t want to train and test on data from the same patient.\n",
        "* Additional Example: In a study analyzing the effectiveness of a new teaching method, if you have multiple test scores from the same classrooms, Group K-Fold would ensure that all scores from a particular classroom are kept together, preventing any classroom's data from appearing in both the training and validation sets.\n",
        "\n",
        "**Leave-One-Group-Out Cross-Validation (LOGO)**\n",
        "* What it is: A specific case of Group K-Fold where you leave out one group at a time as the validation set while using all other groups for training. This is repeated for each group.\n",
        "* When to use it: Use LOGO when you want to assess model performance while ensuring that the model never sees data from a group during training. It’s useful for small datasets or when groups represent distinct entities.\n",
        "* Additional Example: In a clinical trial with multiple patients, if you want to evaluate a model predicting treatment outcomes, LOGO would leave out one patient’s data for validation while training on the remaining patients. This way, the model is tested on completely unseen patient data.\n",
        "\n",
        "**Leave-P-Groups-Out Cross-Validation**\n",
        "* What it is: A generalization of LOGO where you leave out\n",
        "𝑝\n",
        "p groups at a time for validation while using the remaining groups for training. This is done repeatedly for different combinations of groups.\n",
        "* When to use it: Use this method when you have larger datasets and want a more robust evaluation by leaving out multiple groups at once. This helps in assessing how well the model generalizes across multiple related groups.\n",
        "* Additional Example: In a study analyzing customer behavior across different stores, if you have customer purchase data grouped by store locations, you could use Leave-P-Groups-Out to leave out data from several stores for validation while training on the data from the other stores. This allows you to assess how well the model predicts customer behavior in new, unseen store locations."
      ],
      "metadata": {
        "id": "gaTUtqAC8mOH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assumptions"
      ],
      "metadata": {
        "id": "C-kHky4Z9p6f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Assumption for grouped CV:**\n",
        "\n",
        "The dataset contains observations that are grouped together, and it is assumed that data from the same group may be correlated. The goal is to prevent data leakage by ensuring that all observations from a single group are either included in the training set or the validation set, but not both.\n",
        "\n",
        "**Assumption for non group CV:**\n",
        "The dataset consists of independent and identically distributed (i.i.d.) observations. It is assumed that the observations are not related to one another, allowing for random sampling without concern for data leakage."
      ],
      "metadata": {
        "id": "zuXnqbo49sBH"
      }
    }
  ]
}