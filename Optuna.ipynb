{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Important functions in Optuna\n",
        "\n",
        "* create_study: Initializes a new study for hyperparameter optimization, allowing configuration of the optimization process.\n",
        "* sampler: Suggests hyperparameter values for trials based on defined strategies, balancing exploration and exploitation.\n",
        "* pruner: Enables early stopping of trials that are unlikely to yield better results, improving computational efficiency. **By default its TPE**\n",
        "* trials: Represents individual trial runs, capturing all details about hyperparameter configurations and their results for analysis.\n",
        "\n",
        "**Documentation**\n",
        "* [Optuna](https://www.dropbox.com/scl/fo/uthgp753cd46dun5eaorz/AAVbbKcODdsOg-Kjn2blH-I/Section-13-Optuna?dl=0&preview=02-Optuna-Main-Functions.pdf&rlkey=26ugrcmf9wgcz4n28w3ig3jua&subfolder_nav_tracking=1)"
      ],
      "metadata": {
        "id": "jU5y21lIilnJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Algorithms covered\n",
        "\n",
        "* Grid Search\n",
        "* Randomized search\n",
        "* Tree-structured Parzen Estimators\n",
        "* CMA-ES"
      ],
      "metadata": {
        "id": "nFRYm_fBmUY0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the objective function\n"
      ],
      "metadata": {
        "id": "1QfbKg2QmiwJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "import optuna\n",
        "\n",
        "# the objective function takes the hyperparameter space\n",
        "# as input\n",
        "\n",
        "def objective(trial):\n",
        "\n",
        "    rf_n_estimators = trial.suggest_int(\"rf_n_estimators\", 100, 1000)\n",
        "    rf_criterion = trial.suggest_categorical(\"rf_criterion\", ['gini', 'entropy'])\n",
        "    rf_max_depth = trial.suggest_int(\"rf_max_depth\", 1, 4)\n",
        "    rf_min_samples_split = trial.suggest_float(\"rf_min_samples_split\", 0.01, 1)\n",
        "\n",
        "    model = RandomForestClassifier(\n",
        "        n_estimators=rf_n_estimators,\n",
        "        criterion=rf_criterion,\n",
        "        max_depth=rf_max_depth,\n",
        "        min_samples_split=rf_min_samples_split,\n",
        "    )\n",
        "\n",
        "    score = cross_val_score(model, X_train, y_train, cv=3)\n",
        "    accuracy = score.mean()\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "JV8aJcpJi0e2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Search algorithms within Optuna\n"
      ],
      "metadata": {
        "id": "J-NAP5JLnkXn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Create a study for optimization\n",
        "# Initialize an Optuna study to maximize the objective function using random sampling\n",
        "study = optuna.create_study(\n",
        "    direction=\"maximize\",  # Objective is to maximize the performance metric\n",
        "    sampler=optuna.samplers.RandomSampler(),  # Use random sampling for hyperparameter selection\n",
        ")\n",
        "\n",
        "# Step 2: Optimize the objective function\n",
        "# Run the optimization process for a specified number of trials\n",
        "study.optimize(objective, n_trials=20)  # Perform 20 optimization trials\n",
        "\n",
        "# Step 3: Retrieve the best parameters and value\n",
        "# Obtain the hyperparameters that resulted in the highest score\n",
        "study.best_params\n",
        "# Retrieve the best score achieved during the optimization\n",
        "study.best_value\n",
        "# Convert the trial results into a DataFrame for analysis\n",
        "study.trials_dataframe()\n",
        "\n",
        "# Step 4: Create a study using TPE sampling\n",
        "# Initialize an Optuna study for maximizing the objective function with TPE sampling\n",
        "study = optuna.create_study(\n",
        "    direction=\"maximize\",  # Objective is to maximize the performance metric\n",
        "    sampler=optuna.samplers.TPESampler(\n",
        "        n_startup_trials=10  # Number of initial random trials before TPE sampling\n",
        "    ),\n",
        ")\n",
        "\n",
        "# Step 5: Optimize the objective function using TPE\n",
        "# Run the optimization process for a specified number of trials\n",
        "study.optimize(objective, n_trials=20)  # Perform 20 optimization trials\n",
        "# Retrieve the best parameters and value from TPE optimization\n",
        "study.best_params\n",
        "study.best_value\n",
        "\n",
        "# Step 6: Create a study using CMA-ES sampling\n",
        "# Initialize an Optuna study for maximizing the objective function with CMA-ES sampling\n",
        "study = optuna.create_study(\n",
        "    direction=\"maximize\",  # Objective is to maximize the performance metric\n",
        "    sampler=optuna.samplers.CmaEsSampler(),  # Use CMA-ES for hyperparameter selection\n",
        ")\n",
        "\n",
        "# Step 7: Optimize the objective function using CMA-ES\n",
        "# Run the optimization process for a specified number of trials\n",
        "study.optimize(objective, n_trials=5)  # Perform 5 optimization trials\n",
        "# Retrieve the best parameters and value from CMA-ES optimization\n",
        "study.best_params\n",
        "study.best_value\n",
        "\n",
        "# Step 8: Define a search space for grid sampling\n",
        "# Specify the hyperparameter values for grid search\n",
        "search_space = {\n",
        "    \"rf_n_estimators\": [100, 500, 1000],  # Possible values for number of trees\n",
        "    \"rf_criterion\": ['gini', 'entropy'],  # Possible criteria for splitting\n",
        "    \"rf_max_depth\": [1, 2, 3],  # Maximum depth of trees\n",
        "    \"rf_min_samples_split\": [0.1, 1.0]  # Minimum samples required to split an internal node\n",
        "}\n",
        "\n",
        "# Step 9: Create a study using grid sampling\n",
        "# Initialize an Optuna study for maximizing the objective function using grid sampling\n",
        "study = optuna.create_study(\n",
        "    direction=\"maximize\",  # Objective is to maximize the performance metric\n",
        "    sampler=optuna.samplers.GridSampler(search_space),  # Use grid sampling based on defined search space\n",
        ")\n",
        "\n",
        "# Step 10: Optimize the objective function using grid search\n",
        "# Run the optimization process based on the grid search\n",
        "study.optimize(objective)  # Perform optimization using grid sampling\n",
        "# Retrieve the best parameters and value from grid search\n",
        "study.best_params\n",
        "study.best_value"
      ],
      "metadata": {
        "id": "ZMCb73TEmnP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Not recommended to use GridSearch**"
      ],
      "metadata": {
        "id": "m58Q6MMPnccg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tuning hyperparameters for different ML models\n"
      ],
      "metadata": {
        "id": "hmbvyfXPpPgW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier  # Import ensemble classifiers\n",
        "from sklearn.linear_model import LogisticRegression  # Import logistic regression classifier\n",
        "from sklearn.model_selection import cross_val_score  # Import cross-validation score\n",
        "import optuna  # Import Optuna for hyperparameter optimization\n",
        "\n",
        "# Step 1: Define the objective function for optimization\n",
        "# The objective function takes a trial as input to suggest hyperparameters\n",
        "def objective(trial):\n",
        "\n",
        "    # Step 2: Suggest a classifier to optimize\n",
        "    # Select one of the classifiers to optimize from a categorical list\n",
        "    classifier_name = trial.suggest_categorical(\"classifier\", [\"logit\", \"RF\", 'GBM'])\n",
        "\n",
        "    if classifier_name == \"logit\":\n",
        "        # Step 3: Suggest hyperparameters for Logistic Regression\n",
        "        # Select penalty type for logistic regression\n",
        "        logit_penalty = trial.suggest_categorical('logit_penalty', ['l1', 'l2'])\n",
        "        # Select the regularization strength\n",
        "        logit_c = trial.suggest_float('logit_c', 0.001, 10)\n",
        "        logit_solver = 'saga'  # Solver that supports both penalties\n",
        "\n",
        "        # Instantiate Logistic Regression model with selected hyperparameters\n",
        "        model = LogisticRegression(\n",
        "            penalty=logit_penalty,\n",
        "            C=logit_c,\n",
        "            solver=logit_solver,\n",
        "        )\n",
        "\n",
        "    elif classifier_name == \"RF\":\n",
        "        # Step 4: Suggest hyperparameters for Random Forest\n",
        "        # Select number of trees in the forest\n",
        "        rf_n_estimators = trial.suggest_int(\"rf_n_estimators\", 100, 1000)\n",
        "        # Select the criterion for splitting\n",
        "        rf_criterion = trial.suggest_categorical(\"rf_criterion\", ['gini', 'entropy'])\n",
        "        # Select maximum depth of the trees\n",
        "        rf_max_depth = trial.suggest_int(\"rf_max_depth\", 1, 4)\n",
        "        # Select minimum number of samples required to split an internal node\n",
        "        rf_min_samples_split = trial.suggest_float(\"rf_min_samples_split\", 0.01, 1)\n",
        "\n",
        "        # Instantiate Random Forest Classifier model with selected hyperparameters\n",
        "        model = RandomForestClassifier(\n",
        "            n_estimators=rf_n_estimators,\n",
        "            criterion=rf_criterion,\n",
        "            max_depth=rf_max_depth,\n",
        "            min_samples_split=rf_min_samples_split,\n",
        "        )\n",
        "\n",
        "    else:\n",
        "        # Step 5: Suggest hyperparameters for Gradient Boosting Classifier\n",
        "        # Select number of boosting rounds\n",
        "        gbm_n_estimators = trial.suggest_int(\"gbm_n_estimators\", 100, 1000)\n",
        "        # Select the criterion for loss function\n",
        "        gbm_criterion = trial.suggest_categorical(\"gbm_criterion\", ['squared_error', 'friedman_mse'])\n",
        "        # Select maximum depth of the trees\n",
        "        gbm_max_depth = trial.suggest_int(\"gbm_max_depth\", 1, 4)\n",
        "        # Select minimum number of samples required to split an internal node\n",
        "        gbm_min_samples_split = trial.suggest_float(\"gbm_min_samples_split\", 0.01, 1)\n",
        "\n",
        "        # Instantiate Gradient Boosting Classifier model with selected hyperparameters\n",
        "        model = GradientBoostingClassifier(\n",
        "            n_estimators=gbm_n_estimators,\n",
        "            criterion=gbm_criterion,\n",
        "            max_depth=gbm_max_depth,\n",
        "            min_samples_split=gbm_min_samples_split,\n",
        "        )\n",
        "\n",
        "    # Step 6: Evaluate the model using cross-validation\n",
        "    # Perform cross-validation and calculate the accuracy score\n",
        "    score = cross_val_score(model, X_train, y_train, cv=3)  # 3-fold cross-validation\n",
        "    accuracy = score.mean()  # Average accuracy score\n",
        "\n",
        "    return accuracy  # Return the accuracy for optimization\n",
        "\n",
        "# Step 7: Create a study for optimization\n",
        "# Initialize an Optuna study to maximize the accuracy of the model\n",
        "study = optuna.create_study(\n",
        "    direction=\"maximize\",  # Objective is to maximize accuracy\n",
        "    sampler=optuna.samplers.TPESampler(),  # TPE sampler is the default, can be omitted\n",
        ")\n",
        "\n",
        "# Step 8: Optimize the objective function\n",
        "# Run the optimization process for a specified number of trials\n",
        "study.optimize(objective, n_trials=20)  # Perform 20 optimization trials\n",
        "\n",
        "# Step 9: Analyze the results\n",
        "# Convert the study results into a DataFrame\n",
        "results = study.trials_dataframe()\n",
        "\n",
        "# Step 10: Display counts of each classifier used in the trials\n",
        "results['params_classifier'].value_counts()\n",
        "\n",
        "# Step 11: Group the results by classifier and calculate mean and standard deviation of the accuracy\n",
        "results.groupby(['params_classifier'])['value'].agg(['mean', 'std'])"
      ],
      "metadata": {
        "id": "5j1P_pNspQPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The search quickly realised that GBM returned the best performance, so explored the hyperparameter space for that model more than for the others."
      ],
      "metadata": {
        "id": "t03J6JKFvBpr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Optuna Plotting\n"
      ],
      "metadata": {
        "id": "6Z9qYlk14E9d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Plot Optimization History**\n",
        "This plot helps you visualize how the objective (such as accuracy) has changed across different trials. You can use this plot to see how the optimization process progresses and to spot trends or improvements over time.\n",
        "\n",
        "* **Plot Contour** for Specific Parameters\n",
        "A contour plot shows the relationship between two or more parameters and their impact on the objective. It’s useful for visualizing how combinations of parameters affect performance.\n",
        "\n",
        "* **Plot Slice** for Specific Parameters\n",
        "A slice plot shows how the objective value changes as a function of a single hyperparameter, while keeping others fixed. This is useful to see the individual effect of each parameter on the model’s performance.\n",
        "\n",
        "* **Plot Parameter Importances**\n",
        "This plot shows the relative importance of different hyperparameters in the optimization process. You can use it to quickly identify which parameters have the most significant impact on performance.\n",
        "\n",
        "* **Plot Parallel Coordinates** for Selected Parameters\n",
        "A parallel coordinate plot visualizes how each trial performed by showing the relationships between multiple hyperparameters and their impact on the objective. It helps in spotting patterns and identifying successful configurations."
      ],
      "metadata": {
        "id": "RMUbu9anPiee"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna  # Import Optuna for hyperparameter optimization visualization\n",
        "\n",
        "# Step 1: Plot the optimization history\n",
        "# Create a figure showing the optimization history of the study\n",
        "fig = optuna.visualization.matplotlib.plot_optimization_history(study)\n",
        "\n",
        "# Step 2: Plot contour for specific parameters\n",
        "# Create a contour plot for the relationship between selected parameters\n",
        "optuna.visualization.matplotlib.plot_contour(\n",
        "    study,\n",
        "    params=[\"num_conv_layers\", \"num_dense_layers\", \"optimizer_name\", 'units'],\n",
        ")\n",
        "\n",
        "# Step 3: Plot the slice of the study\n",
        "# Create a slice plot to visualize how the objective value changes with the selected parameters\n",
        "optuna.visualization.matplotlib.plot_slice(\n",
        "    study,\n",
        "    params=[\"num_conv_layers\", \"num_dense_layers\", \"optimizer_name\", 'units'],\n",
        ")\n",
        "\n",
        "# Step 4: Plot parameter importances\n",
        "# Create a plot to visualize the importance of each hyperparameter in the study\n",
        "optuna.visualization.matplotlib.plot_param_importances(study)\n",
        "\n",
        "# Step 5: Plot parallel coordinate for selected parameters\n",
        "# Create a parallel coordinate plot to visualize the relationship between parameters and objective value\n",
        "optuna.visualization.matplotlib.plot_parallel_coordinate(\n",
        "    study,\n",
        "    params=[\"num_conv_layers\", \"num_dense_layers\", \"optimizer_name\", 'units'],\n",
        ")"
      ],
      "metadata": {
        "id": "9wKF8UP14FU5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**For better overview of how to plot**\n",
        "\n",
        "* [Plotting](https://github.com/solegalli/hyperparameter-optimization/blob/master/Section-12-Optuna/05-Evaluating-the-search.ipynb)"
      ],
      "metadata": {
        "id": "BLRxuFhI4n-l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Successive Halving with Optuna\n"
      ],
      "metadata": {
        "id": "aCYLLwVf49sx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb  # Import the XGBoost library for gradient boosting\n",
        "\n",
        "def objective(trial):\n",
        "    # Step 1: Define the hyperparameter space\n",
        "    # Set up initial hyperparameters for the XGBoost model\n",
        "    param = {\n",
        "        \"verbosity\": 0,  # Suppress output messages\n",
        "        \"objective\": \"binary:logistic\",  # Specify binary logistic regression for binary classification\n",
        "        \"eval_metric\": \"auc\",  # Use AUC as the evaluation metric\n",
        "        \"booster\": trial.suggest_categorical(\"booster\", [\"gbtree\", \"gblinear\", \"dart\"]),  # Choose the type of booster\n",
        "        \"lambda\": trial.suggest_float(\"lambda\", 1e-8, 1.0, log=True),  # L2 regularization term\n",
        "        \"alpha\": trial.suggest_float(\"alpha\", 1e-8, 1.0, log=True),  # L1 regularization term\n",
        "    }\n",
        "\n",
        "    # Step 2: Conditional hyperparameter space\n",
        "    # Additional hyperparameters that depend on the booster type\n",
        "    if param[\"booster\"] == \"gbtree\" or param[\"booster\"] == \"dart\":\n",
        "        param[\"max_depth\"] = trial.suggest_int(\"max_depth\", 1, 9)  # Maximum depth of the trees\n",
        "        param[\"eta\"] = trial.suggest_float(\"eta\", 1e-8, 1.0, log=True)  # Learning rate\n",
        "        param[\"gamma\"] = trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True)  # Minimum loss reduction\n",
        "        param[\"grow_policy\"] = trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"])  # Growth policy for tree construction\n",
        "\n",
        "    if param[\"booster\"] == \"dart\":\n",
        "        param[\"sample_type\"] = trial.suggest_categorical(\"sample_type\", [\"uniform\", \"weighted\"])  # Sampling type for DART\n",
        "        param[\"normalize_type\"] = trial.suggest_categorical(\"normalize_type\", [\"tree\", \"forest\"])  # Normalization type for DART\n",
        "        param[\"rate_drop\"] = trial.suggest_float(\"rate_drop\", 1e-8, 1.0, log=True)  # Dropout rate for DART\n",
        "        param[\"skip_drop\"] = trial.suggest_float(\"skip_drop\", 1e-8, 1.0, log=True)  # Skip dropout rate for DART\n",
        "\n",
        "    # Step 3: Add pruning callback\n",
        "    # Set up pruning callback to stop unpromising trials early based on validation AUC\n",
        "    pruning_callback = optuna.integration.XGBoostPruningCallback(trial, \"validation-auc\")\n",
        "\n",
        "    # Step 4: Train the model\n",
        "    # Train the XGBoost model with the specified parameters and evaluation dataset\n",
        "    bst = xgb.train(param, dtrain, evals=[(dtest, \"validation\")], callbacks=[pruning_callback])\n",
        "\n",
        "    # Step 5: Evaluate the model\n",
        "    # Make predictions and calculate accuracy\n",
        "    preds = bst.predict(dtest)  # Get predictions for the test dataset\n",
        "    pred_labels = np.rint(preds)  # Round predictions to get binary labels\n",
        "    accuracy = accuracy_score(y_test, pred_labels)  # Calculate accuracy score\n",
        "\n",
        "    return accuracy  # Return accuracy for optimization\n",
        "\n",
        "# Step 6: Create a study for optimization\n",
        "# Initialize an Optuna study to maximize accuracy of the model\n",
        "study = optuna.create_study(\n",
        "    sampler=optuna.samplers.RandomSampler(),  # Use random sampling for hyperparameter selection\n",
        "    pruner=optuna.pruners.SuccessiveHalvingPruner(  # Set up a pruning strategy for trials\n",
        "        min_resource=1,  # Minimum validation rounds before stopping\n",
        "        reduction_factor=3,  # Factor by which to reduce resources\n",
        "        bootstrap_count=0,  # Minimum number of trials to complete before promoting\n",
        "    ),\n",
        "    direction=\"maximize\",  # Objective is to maximize accuracy\n",
        ")\n",
        "\n",
        "# Step 7: Optimize the objective function\n",
        "# Run the optimization process for a specified number of trials\n",
        "study.optimize(\n",
        "    objective,\n",
        "    n_trials=30,  # Perform 30 optimization trials\n",
        ")"
      ],
      "metadata": {
        "id": "4VmlwrN_4z1V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Additional notes**\n",
        "\n",
        "* Conditional Dependencies: The hyperparameters that are defined depend on the choice of the boosting method (booster). For instance, if you choose \"gblinear\", you won’t need to set parameters like max_depth or eta, as they are irrelevant for linear models. This flexibility allows the model to adapt to the chosen boosting method, optimizing its performance based on the specific characteristics of that method.\n",
        "* By structuring the hyperparameter space this way, you ensure that only the relevant hyperparameters are suggested and optimized during the training process, which simplifies the optimization problem and improves model performance.\n",
        "\n",
        "**Nested Hyperparameters**\n",
        "* The first if block is applied to both gbtree and dart. These two boosters share certain hyperparameters, such as max_depth, eta, gamma, and grow_policy.\n",
        "* The second if block specifically targets only dart because dart has additional hyperparameters (sample_type, normalize_type, rate_drop, and skip_drop) that are not relevant to gbtree.\n",
        "\n",
        "If you remove the \"or\" condition from the first if block, dart would not get the shared hyperparameters (max_depth, eta, etc.), which are still necessary for its performance. Here's a breakdown:"
      ],
      "metadata": {
        "id": "AG19UpFR7whx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperband with Optuna\n"
      ],
      "metadata": {
        "id": "IIAtkJhmH-4X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb  # Import XGBoost for the model\n",
        "\n",
        "# Step 1: Create an Optuna study\n",
        "# Initialize a study for optimizing hyperparameters\n",
        "study = optuna.create_study(\n",
        "\n",
        "    # Step 2: Define the sampling method for hyperparameters\n",
        "    # RandomSampler randomly samples hyperparameters\n",
        "    sampler=optuna.samplers.RandomSampler(),\n",
        "\n",
        "    # Step 3: Use a pruner to stop unpromising trials early\n",
        "    # HyperbandPruner is used to prune less promising configurations during training\n",
        "    pruner=optuna.pruners.HyperbandPruner(\n",
        "\n",
        "        # Step 4: Minimum resource\n",
        "        # Minimum number of validation rounds required before pruning\n",
        "        min_resource=1,\n",
        "\n",
        "        # Step 5: Maximum resource\n",
        "        # The maximum budget or the total number of validation rounds\n",
        "        max_resource=81,\n",
        "\n",
        "        # Step 6: Reduction factor\n",
        "        # Determines how many configurations will be promoted to the next round\n",
        "        reduction_factor=3,\n",
        "    ),\n",
        "\n",
        "    # Step 7: Direction of optimization\n",
        "    # We aim to maximize the objective value (e.g., accuracy)\n",
        "    direction=\"maximize\",\n",
        ")\n",
        "\n",
        "# Step 8: Optimize the objective function\n",
        "# Perform optimization, limiting the number of trials (hyperparameter configurations)\n",
        "study.optimize(\n",
        "    objective,\n",
        "\n",
        "    # Step 9: Number of trials\n",
        "    # The number of hyperparameter configurations to try\n",
        "    n_trials=50,\n",
        ")"
      ],
      "metadata": {
        "id": "8QotWbkoH_TM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Specific parameters description"
      ],
      "metadata": {
        "id": "fp5aJjKRPTh_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Minimum resource\n",
        "# This defines the smallest amount of resources (e.g., training steps, epochs, or validation rounds)\n",
        "# that must be used before the pruner considers stopping a trial. Setting it to 1 means that\n",
        "# every trial will run at least one validation round before any pruning decisions are made.\n",
        "min_resource=1,\n",
        "\n",
        "# Step 5: Maximum resource\n",
        "# This sets the total available budget for training, often in terms of the number of validation rounds\n",
        "# or epochs. In this case, the training process will be allowed to run up to 81 validation rounds (or units of training).\n",
        "# Trials may be stopped early based on performance, but none will exceed this limit.\n",
        "max_resource=81,\n",
        "\n",
        "# Step 6: Reduction factor\n",
        "# This controls how aggressively trials are pruned. A reduction factor of 3 means that\n",
        "# after each stage, only about one-third of the remaining configurations will continue to the next round.\n",
        "# The rest will be pruned (stopped) to focus the computational resources on the more promising trials.\n",
        "reduction_factor=3,\n",
        "\n",
        "# Step 7: Bootstrap count\n",
        "# This parameter defines the minimum number of trials that must complete a stage (rung)\n",
        "# before any trials are promoted to the next stage. Setting `bootstrap_count=0` allows\n",
        "# trials to be promoted as soon as they finish without waiting for others.\n",
        "# This speeds up the optimization process but might lead to premature promotions based on limited data.\n",
        "bootstrap_count=0,"
      ],
      "metadata": {
        "id": "K7arTvdVKvxO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}