{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperopt Definitions"
      ],
      "metadata": {
        "id": "O49lAH8A9Ftu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hyperopt: Space Configuration**\n",
        "One of the most valuable offers of Hyperopt is the flexibility it provides to create priors over the hyperparameters distributions.\n",
        "\n",
        "**Hyperopt offers**:\n",
        "* Multiple distributions\n",
        "* Possibility to combine distributions\n",
        "* Possibility to create nested spaces\n",
        "* Multiple configuration ways including lists, dictionaries and tuples\n",
        "\n",
        "**Distributions**\n",
        "\n",
        "* hp.choice: returns one of several options (suitable for categorical hyperparams)\n",
        "* hp.randint: returns a random integer between 0 and an upper limit\n",
        "* hp.uniform: returns a value uniformly between specified limits\n",
        "* hp.quniform: Returns a value like round(uniform(low, high) / q) * q\n",
        "\n",
        "**hp.quniform** would be an equivalent of randint (if q=1), but the upper and **lower** limits can be specified. hp.quniform also offers the possibility to use bigger values of q. So if we search for the optimal number of trees in a random forest, we could search hp.quniform('n_estimators', 10, 1000, 50), in which case we would sample between 10 and 1000 trees in increments of 50."
      ],
      "metadata": {
        "id": "viusi3XIjECZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Basic Syntax"
      ],
      "metadata": {
        "id": "V1RsuQhM9Wmt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing hp module for hyperparameter space definitions\n",
        "# This module provides functions to define various types of search spaces for hyperparameter tuning.\n",
        "from hyperopt import hp\n",
        "# Importing sample function for sampling from distributions\n",
        "from hyperopt.pyll.stochastic import sample"
      ],
      "metadata": {
        "id": "9RLZFBBEG8SH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Integer and Continuous Sampling:\n"
      ],
      "metadata": {
        "id": "kZDmUgPyG-CC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This space samples an integer uniformly from [0, 10)\n",
        "# This allows for selecting integer hyperparameters within a specific range randomly.\n",
        "# Example: Choosing a random 'max_leaf_nodes' value for a decision tree.\n",
        "space = [{'example': hp.randint('example', 10)}]\n",
        "\n",
        "# This space samples a float uniformly from [1, 10]\n",
        "# This allows for continuous hyperparameters to be sampled within a range.\n",
        "# Example: Selecting a float value for 'max_features' that determines the number of features to consider at each split in a decision tree.\n",
        "space = [{'example': hp.uniform('example', 1, 10)}]\n",
        "\n",
        "# This space samples a float uniformly from [1, 10] at discrete intervals of 1\n",
        "# This is useful for sampling continuous values but constrained to certain intervals.\n",
        "# Example: Sampling the 'min_samples_leaf' in a decision tree, ensuring it can only take whole number values.\n",
        "space = [{'example': hp.quniform('example', 1, 10, 1)}]"
      ],
      "metadata": {
        "id": "loG4_qVnHAOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Logarithmic Sampling:\n"
      ],
      "metadata": {
        "id": "z4lp4RMzHEok"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This space samples a float from a log-uniform distribution between [1, 10]\n",
        "# This method is used when we expect the optimal values to be better expressed on a logarithmic scale.\n",
        "# Example: Setting 'alpha' in a regularized decision tree model, where smaller values are more common.\n",
        "space = [{'example': hp.loguniform('example', 1, 10)}]\n",
        "\n",
        "# This space samples from a log-uniform distribution between [10, 1000] in increments of 50\n",
        "# This samples values in a way that is biased towards smaller values, useful for certain distributions.\n",
        "# Example: Tuning the 'max_depth' of a decision tree with an emphasis on shallower trees.\n",
        "space = [{'example': hp.qloguniform('example', np.log(10), np.log(1000), np.log(50))}]"
      ],
      "metadata": {
        "id": "TP-5vO61HGlR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Normal Distribution Sampling:\n"
      ],
      "metadata": {
        "id": "pdHWOfffHHYO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This space samples from a standard normal distribution (mean=0, std=1)\n",
        "# This allows for sampling continuous values with a specified normal distribution.\n",
        "# Example: Adjusting 'max_features' based on a normal distribution around a preferred value.\n",
        "space = [{'example': hp.normal('example', 0, 1)}]\n",
        "\n",
        "# This space samples from a normal distribution (mean=100, std=25) at discrete intervals of 10\n",
        "# This provides samples that are rounded to specific intervals, useful for hyperparameters.\n",
        "# Example: Sampling 'min_samples_split' for a decision tree, ensuring it only takes discrete values.\n",
        "space = [{'example': hp.qnormal('example', 100, 25, 10)}]\n",
        "\n",
        "# This space samples from a log-normal distribution (mean=np.log(100), std=np.log(25))\n",
        "# This method is used for parameters that are strictly positive and follow a log-normal distribution.\n",
        "# Example: Setting 'min_impurity_decrease' in a decision tree to ensure itâ€™s always a positive value.\n",
        "space = [{'example': hp.lognormal('example', np.log(100), np.log(25))}]\n",
        "\n",
        "# This space samples from a log-normal distribution at discrete intervals\n",
        "# This samples values that follow a log-normal distribution but constrained to specific intervals.\n",
        "# Example: Tuning 'max_depth' of a decision tree while ensuring the values are rounded.\n",
        "space = [{'example': hp.qlognormal('example', 0, 1, 1)}]"
      ],
      "metadata": {
        "id": "jO3tEbILHJNo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Categorical and Probabilistic Sampling:\n"
      ],
      "metadata": {
        "id": "066q7qYvHLiX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This space samples from a predefined list of options: 'A', 'B', or 'C'\n",
        "# This allows for selecting categorical hyperparameters from a fixed set of choices.\n",
        "# Example: Choosing a specific criterion ('gini' or 'entropy') for splitting in a decision tree.\n",
        "space = [{'example': hp.choice('example', ['A', 'B', 'C'])}]\n",
        "\n",
        "# This space samples a loss function based on given probabilities\n",
        "# 80% chance for 'deviance' and 20% for 'exponential'\n",
        "# This method allows for sampling based on defined probabilities, which is useful in decision-making scenarios.\n",
        "# Example: Choosing a loss function for training a decision tree based on their effectiveness in the context.\n",
        "space = hp.pchoice('example', [\n",
        "    (0.8, {'loss': 'deviance'}),\n",
        "    (0.2, {'loss': 'exponential'})\n",
        "])"
      ],
      "metadata": {
        "id": "v70QapknHMQ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Nested Hyperparameters"
      ],
      "metadata": {
        "id": "Fxb9A2oc9YOS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Define a hyperparameter space for selecting different classifier types\n",
        "space = hp.choice('classifier_type', [\n",
        "\n",
        "    # Step 2: Define the naive_bayes classifier configuration\n",
        "    {\n",
        "        'type': 'naive_bayes',\n",
        "    },\n",
        "\n",
        "    # Step 3: Define the SVM classifier configuration with hyperparameters\n",
        "    {\n",
        "        'type': 'svm',\n",
        "\n",
        "        # Step 4: Define the regularization parameter C for SVM sampled from a log-normal distribution\n",
        "        'C': hp.lognormal('svm_C', 0, 1),\n",
        "\n",
        "        # Step 5: Define the kernel type for SVM with options for linear and RBF kernels\n",
        "        'kernel': hp.choice('svm_kernel', [\n",
        "            # Step 6: Define the linear kernel configuration\n",
        "            {'ktype': 'linear'},\n",
        "\n",
        "            # Step 7: Define the RBF kernel configuration with a width parameter sampled from a log-normal distribution\n",
        "            {'ktype': 'RBF', 'width': hp.lognormal('svm_rbf_width', 0, 1)},\n",
        "        ]),\n",
        "    },\n",
        "\n",
        "    # Step 8: Define the decision tree classifier configuration with hyperparameters\n",
        "    {\n",
        "        'type': 'dtree',\n",
        "\n",
        "        # Step 9: Define the criterion for the decision tree with options for 'gini' and 'entropy'\n",
        "        'criterion': hp.choice('dtree_criterion', ['gini', 'entropy']),\n",
        "\n",
        "        # Step 10: Define the maximum depth for the decision tree with an option for None and a sampled value\n",
        "        'max_depth': hp.choice('dtree_max_depth',\n",
        "            [None, hp.qlognormal('dtree_max_depth_int', 3, 1, 1)]),\n",
        "\n",
        "        # Step 11: Define the minimum samples required to split an internal node, sampled from a log-normal distribution\n",
        "        'min_samples_split': hp.qlognormal('dtree_min_samples_split', 2, 1, 1),\n",
        "    },\n",
        "])"
      ],
      "metadata": {
        "id": "5DLzIQMEjZU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Search algorithms within Hyperopt\n"
      ],
      "metadata": {
        "id": "ulOxpfhA9gqC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperopt provides 3 search algorithms:\n",
        "* Randomized search\n",
        "* Annealing\n",
        "* Tree-structured Parzen Estimators\n",
        "\n",
        "I find the documentation for Hyperopt quite unintuitive, so it helps to refer to the original article to understand the different parameters and classes.\n",
        "\n",
        "Procedure\n",
        "To tune the hyper-parameters of our model we need to:\n",
        "\n",
        "* define a model\n",
        "* define the hyperparameter space\n",
        "* define the objective function we want to minimize.\n",
        "* Run the minimization"
      ],
      "metadata": {
        "id": "FTv0VtlX9hrk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import XGBoost package\n",
        "# Load XGBoost library to use its functionality\n",
        "import xgboost as xgb\n",
        "\n",
        "# hp: define the hyperparameter space\n",
        "# fmin: optimization function\n",
        "# Trials: to evaluate the different searched hyperparameters\n",
        "from hyperopt import hp, fmin\n",
        "# the search algorithms\n",
        "from hyperopt import rand, anneal, tpe\n",
        "\n",
        "# Step 2: Define the hyperparameter space using a parameter grid\n",
        "# Creates a dictionary that specifies the range of hyperparameters for tuning\n",
        "param_grid = {\n",
        "    'n_estimators': hp.quniform('n_estimators', 200, 2500, 100),  # Number of boosting rounds\n",
        "    'max_depth': hp.quniform('max_depth', 1, 10, 1),  # Maximum tree depth\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.001), np.log(1)),  # Learning rate\n",
        "    'booster': hp.choice('booster', ['gbtree', 'dart']),  # Booster type\n",
        "    'gamma': hp.loguniform('gamma', np.log(0.01), np.log(10)),  # Minimum loss reduction\n",
        "    'subsample': hp.uniform('subsample', 0.50, 0.90),  # Subsample ratio of training data\n",
        "    'colsample_bytree': hp.uniform('colsample_bytree', 0.50, 0.99),  # Subsample ratio of features per tree\n",
        "    'colsample_bylevel': hp.uniform('colsample_bylevel', 0.50, 0.99),  # Subsample ratio of features per level\n",
        "    'colsample_bynode': hp.uniform('colsample_bynode', 0.50, 0.99),  # Subsample ratio of features per node\n",
        "    'reg_lambda': hp.uniform('reg_lambda', 1, 20)  # L2 regularization term\n",
        "}\n",
        "\n",
        "# Step 3: Perform random search optimization\n",
        "# Uses random search to minimize the objective function over the hyperparameter space\n",
        "random_search = fmin(\n",
        "    fn=objective,  # Function to minimize\n",
        "    space=param_grid,  # Hyperparameter space\n",
        "    max_evals=50,  # Maximum evaluations\n",
        "    rstate=np.random.default_rng(42),  # Random state for reproducibility\n",
        "    algo=rand.suggest  # Random search algorithm\n",
        ")\n",
        "random_search\n",
        "\n",
        "# Step 4: Perform annealing search optimization\n",
        "# Uses simulated annealing to minimize the objective function over the hyperparameter space\n",
        "anneal_search = fmin(\n",
        "    fn=objective,  # Function to minimize\n",
        "    space=param_grid,  # Hyperparameter space\n",
        "    max_evals=50,  # Maximum evaluations\n",
        "    rstate=np.random.default_rng(42),  # Random state for reproducibility\n",
        "    algo=anneal.suggest  # Annealing search algorithm\n",
        ")\n",
        "anneal_search\n",
        "\n",
        "# Step 5: Perform Tree-structured Parzen Estimator (TPE) search optimization\n",
        "# Uses TPE to minimize the objective function over the hyperparameter space\n",
        "tpe_search = fmin(\n",
        "    fn=objective,  # Function to minimize\n",
        "    space=param_grid,  # Hyperparameter space\n",
        "    max_evals=50,  # Maximum evaluations\n",
        "    rstate=np.random.default_rng(42),  # Random state for reproducibility\n",
        "    algo=tpe.suggest  # TPE algorithm\n",
        ")\n",
        "tpe_search"
      ],
      "metadata": {
        "id": "aWtxaWxM9hFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conditional Search Spaces with Hyperopt\n",
        "In this notebooks, we will create a conditional (or nested) hyperparameter space to optimize the hyperparameters of 3 different off-the-shelf algorithms.\n",
        "\n",
        "Utilizing the breast cancer dataset, we will optimize the hyperparameters for a linear regression, a random forest and a gradient boosting machine within the same search."
      ],
      "metadata": {
        "id": "y3Ho2cjeJ25x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "# hp: define the hyperparameter space\n",
        "# fmin: optimization function\n",
        "# Trials: to evaluate the different searched hyperparameters\n",
        "from hyperopt import hp, fmin, Trials\n",
        "\n",
        "# the search algorithms\n",
        "from hyperopt import anneal\n",
        "\n",
        "# Step 1: Define the hyperparameter space for multiple classifiers\n",
        "# We define a list of dictionaries with hyperparameters for different models\n",
        "param_grid = hp.choice('classifier', [\n",
        "\n",
        "    # Step 2: Logistic Regression model\n",
        "    # Specify hyperparameter space for Logistic Regression\n",
        "    {'model': LogisticRegression,  # Model class\n",
        "    'params': {\n",
        "        'penalty': hp.choice('penalty', ['l1', 'l2']),  # Regularization penalty\n",
        "        'C': hp.uniform('C', 0.001, 10),  # Inverse of regularization strength\n",
        "        'solver': 'saga',  # Solver that supports both penalties\n",
        "    }},\n",
        "\n",
        "    # Step 3: Random Forest Classifier model\n",
        "    # Specify hyperparameter space for Random Forest\n",
        "    {'model': RandomForestClassifier,  # Model class\n",
        "    'params': {\n",
        "        'n_estimators': hp.quniform('n_estimators_rf', 50, 1500, 50),  # Number of trees\n",
        "        'max_depth': hp.quniform('max_depth_rf', 1, 5, 1),  # Maximum depth of trees\n",
        "        'criterion': hp.choice('criterion_rf', ['gini', 'entropy']),  # Split quality criterion\n",
        "    }},\n",
        "\n",
        "    # Step 4: Gradient Boosting Classifier model\n",
        "    # Specify hyperparameter space for Gradient Boosting\n",
        "    {'model': GradientBoostingClassifier,  # Model class\n",
        "    'params': {\n",
        "        'n_estimators': hp.quniform('n_estimators_gbm', 50, 1500, 50),  # Number of boosting rounds\n",
        "        'max_depth': hp.quniform('max_depth_gbm', 1, 5, 1),  # Maximum tree depth\n",
        "        'criterion': hp.choice('criterion_gbm', ['friedman_mse', 'squared_error']),  # Loss function criterion\n",
        "    }},\n",
        "])"
      ],
      "metadata": {
        "id": "9Y6B-77dJ8i3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Objective Functions"
      ],
      "metadata": {
        "id": "3g80K7mSRk4t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def objective(params):\n",
        "\n",
        "    # Step 1: Instantiate the model\n",
        "    # Create a new instance of the model class based on the sampled hyperparameters\n",
        "    model = params['model']()  # Don't forget the () to instantiate the class\n",
        "\n",
        "    # Step 2: Capture the sampled hyperparameters\n",
        "    # Extract the hyperparameters from the params dictionary\n",
        "    hyperparams = params['params']\n",
        "\n",
        "    try:\n",
        "        # Step 3: Convert n_estimators and max_depth to integers\n",
        "        # Ensure that tree-based algorithm parameters are integer values\n",
        "        hyperparams['n_estimators'] = int(hyperparams['n_estimators'])\n",
        "        hyperparams['max_depth'] = int(hyperparams['max_depth'])\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    # Step 4: Print the model and hyperparameters for visualization (optional)\n",
        "    # Displays the current model and hyperparameters being used for tuning\n",
        "    print(model, hyperparams)\n",
        "\n",
        "    # Step 5: Set the sampled hyperparameters to the model\n",
        "    # Apply the sampled hyperparameters to the model using set_params\n",
        "    model.set_params(**hyperparams)\n",
        "\n",
        "    # Step 6: Train the model using cross-validation\n",
        "    # Perform 3-fold cross-validation and calculate accuracy\n",
        "    cross_val_data = cross_val_score(\n",
        "        model,\n",
        "        X_train,  # Training data features\n",
        "        y_train,  # Training data labels\n",
        "        scoring='accuracy',  # Accuracy as the performance metric\n",
        "        cv=3,  # 3-fold cross-validation\n",
        "        n_jobs=4,  # Run jobs in parallel using 4 CPU cores\n",
        "    )\n",
        "\n",
        "    # Step 7: Calculate loss\n",
        "    # Negate the average cross-validation accuracy to minimize the loss\n",
        "    loss = -cross_val_data.mean()\n",
        "    print(loss)\n",
        "    print()\n",
        "\n",
        "    # Step 8: Return the loss\n",
        "    # Return the calculated loss to the optimization function\n",
        "    return loss"
      ],
      "metadata": {
        "id": "ooSD--3EQZqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Evaluation"
      ],
      "metadata": {
        "id": "fjrsbMfdSI0S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Initialize the Trials object\n",
        "# Trials object stores details of each evaluation during the optimization process\n",
        "trials = Trials()\n",
        "\n",
        "# Step 2: Perform annealing search optimization\n",
        "# Use the anneal algorithm to minimize the objective function over the hyperparameter space\n",
        "anneal_search = fmin(\n",
        "    fn=objective,  # Function to minimize\n",
        "    space=param_grid,  # Hyperparameter space\n",
        "    max_evals=50,  # Maximum number of evaluations\n",
        "    rstate=np.random.default_rng(42),  # Random state for reproducibility\n",
        "    algo=anneal.suggest,  # Annealing search algorithm\n",
        "    trials=trials  # Store the trials for later analysis\n",
        ")\n",
        "\n",
        "# Step 3: Return the best hyperparameters found by the anneal search\n",
        "# Displays the hyperparameters that achieved the lowest loss\n",
        "anneal_search\n",
        "\n",
        "# Step 4: Return the hyperparameters with the minimum loss\n",
        "# Returns the specific combination of hyperparameters that achieved the lowest loss\n",
        "trials.argmin\n",
        "\n",
        "# Step 5: Calculate the average best error from the trials/accuracy\n",
        "# Computes the average error from the best trials during the optimization process\n",
        "trials.average_best_error()\n",
        "\n",
        "# Step 6: Return the best trial from the optimization process\n",
        "# Provides detailed information about the best trial during the search\n",
        "trials.best_trial"
      ],
      "metadata": {
        "id": "Mj8PjTbWQcZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overall, trials.best_trial gives you detailed information about the best-performing trial in your hyperparameter optimization process, including the trial's unique ID, loss value, status, and the specific hyperparameters used. This is useful for understanding what settings yielded the best results and for potential deployment or further analysis."
      ],
      "metadata": {
        "id": "dxjEgzGheOMk"
      }
    }
  ]
}